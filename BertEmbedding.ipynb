{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muradbozik/.pyenv/versions/3.7.7/envs/sparknlp/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import sparknlp # nlp processing\n",
    "from sklearn.model_selection import train_test_split # splitting data\n",
    "\n",
    "import matplotlib.pyplot as plt # visualisation\n",
    "import seaborn as sns # visualisation \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomState = np.random.RandomState(seed=42) # for creating same randomness in each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  2.5.0\n",
      "Apache Spark version:  2.4.4\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- qid1: string (nullable = true)\n",
      " |-- qid2: string (nullable = true)\n",
      " |-- question1: string (nullable = true)\n",
      " |-- question2: string (nullable = true)\n",
      " |-- is_duplicate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(spark)\n",
    "\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dataset/clean_data.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_partial_pipeline(column):\n",
    "    document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(column) \\\n",
    "        .setOutputCol(column+\"_document\")\\\n",
    "        .setCleanupMode(\"shrink\") \n",
    "    \n",
    "    sentence_detector = SentenceDetector() \\\n",
    "        .setInputCols([column+\"_document\"]) \\\n",
    "        .setOutputCol(column+\"_sentence\") \\\n",
    "        .setUseAbbreviations(True)\n",
    "    \n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([column+\"_sentence\"]) \\\n",
    "        .setOutputCol(column+\"_token\")\n",
    "    \n",
    "    spell_checker = NorvigSweetingApproach() \\\n",
    "        .setInputCols([column+\"_token\"]) \\\n",
    "        .setOutputCol(column+\"_checked\") \\\n",
    "        .setDictionary(\"./spell/coca2017.txt\", \"[a-zA-Z]+\")\n",
    "    \n",
    "    normalizer = Normalizer() \\\n",
    "        .setInputCols([column+\"_checked\"]) \\\n",
    "        .setOutputCol(column+\"_normalized\")\n",
    "    \n",
    "    lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
    "        .setInputCols([column+\"_normalized\"]) \\\n",
    "        .setOutputCol(column+\"_lemma\")\n",
    "    \n",
    "    stopwords_cleaner = StopWordsCleaner()\\\n",
    "        .setInputCols(column+\"_lemma\")\\\n",
    "        .setOutputCol(column+\"_cleanTokens\")\\\n",
    "        .setCaseSensitive(False)\n",
    "   \n",
    "    finisher = Finisher() \\\n",
    "        .setInputCols([column+\"_cleanTokens\"]) \\\n",
    "        .setOutputCols([column+\"_finished\"])\\\n",
    "        .setIncludeMetadata(False)\\\n",
    "        .setCleanAnnotations(True)\n",
    "    \n",
    "    return [document_assembler, sentence_detector, tokenizer, spell_checker, normalizer, lemma, stopwords_cleaner, finisher]\n",
    "\n",
    "def preprocessing_pipeline():\n",
    "\n",
    "    q1_stages = preprocessing_partial_pipeline(\"question1\")\n",
    "    \n",
    "    q2_stages = preprocessing_partial_pipeline(\"question2\")\n",
    "     \n",
    "    pipeline = Pipeline(stages=q1_stages+q2_stages)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limited = df.limit(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='0', qid1='1', qid2='2', question1='What is the step by step guide to invest in share market in india?', question2='What is the step by step guide to invest in share market?', is_duplicate='0'),\n",
       " Row(id='1', qid1='3', qid2='4', question1='What is the story of Kohinoor (Koh-i-Noor) Diamond?', question2='What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?', is_duplicate='0'),\n",
       " Row(id='2', qid1='5', qid2='6', question1='How can I increase the speed of my internet connection while using a VPN?', question2='How can Internet speed be increased by hacking through DNS?', is_duplicate='0'),\n",
       " Row(id='3', qid1='7', qid2='8', question1='Why am I mentally very lonely? How can I solve it?', question2='Find the remainder when [math]23^{24}[/math] is divided by 24,23?', is_duplicate='0'),\n",
       " Row(id='4', qid1='9', qid2='10', question1='Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?', question2='Which fish would survive in salt water?', is_duplicate='0')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before Preprocessing \n",
    "df_limited.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pre_pipeline = preprocessing_pipeline()\n",
    "model = pre_pipeline.fit(df_limited)\n",
    "df_processed = model.transform(df_limited).persist().select(\"question1\", \"question2\", \"question1_finished\", \"question2_finished\", \"is_duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(question1='What is the step by step guide to invest in share market in india?', question2='What is the step by step guide to invest in share market?', question1_finished=['step', 'step', 'guide', 'invest', 'share', 'market', 'india'], question2_finished=['step', 'step', 'guide', 'invest', 'share', 'market'], is_duplicate='0'),\n",
       " Row(question1='What is the story of Kohinoor (Koh-i-Noor) Diamond?', question2='What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?', question1_finished=['story', 'kohinoor', 'KohiNoor', 'diamond'], question2_finished=['happen', 'indian', 'government', 'steal', 'kohinoor', 'KohiNoor', 'diamond', 'back'], is_duplicate='0'),\n",
       " Row(question1='How can I increase the speed of my internet connection while using a VPN?', question2='How can Internet speed be increased by hacking through DNS?', question1_finished=['increase', 'speed', 'internet', 'connection', 'use', 'VPN'], question2_finished=['internet', 'speed', 'increase', 'hack', 'DNS'], is_duplicate='0'),\n",
       " Row(question1='Why am I mentally very lonely? How can I solve it?', question2='Find the remainder when [math]23^{24}[/math] is divided by 24,23?', question1_finished=['mentally', 'lonely', 'solve'], question2_finished=['wind', 'remainder', 'mathmath', 'divide'], is_duplicate='0'),\n",
       " Row(question1='Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?', question2='Which fish would survive in salt water?', question1_finished=['one', 'dissolve', 'water', 'quikly', 'sugar', 'salt', 'methane', 'carbon', 'di', 'oxide'], question2_finished=['fish', 'survive', 'salt', 'water'], is_duplicate='0')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing + Bert Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_partial(column):\n",
    "    document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(column) \\\n",
    "        .setOutputCol(column+\"_document\")\\\n",
    "        .setCleanupMode(\"shrink\") \n",
    "    \n",
    "    sentence_detector = SentenceDetector() \\\n",
    "        .setInputCols([column+\"_document\"]) \\\n",
    "        .setOutputCol(column+\"_sentence\") \\\n",
    "        .setUseAbbreviations(True)\n",
    "    \n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([column+\"_sentence\"]) \\\n",
    "        .setOutputCol(column+\"_token\")\n",
    "    \n",
    "    spell_checker = NorvigSweetingApproach() \\\n",
    "        .setInputCols([column+\"_token\"]) \\\n",
    "        .setOutputCol(column+\"_checked\") \\\n",
    "        .setDictionary(\"./spell/coca2017.txt\", \"[a-zA-Z]+\")\n",
    "    \n",
    "    normalizer = Normalizer() \\\n",
    "        .setInputCols([column+\"_checked\"]) \\\n",
    "        .setOutputCol(column+\"_normalized\")\n",
    "    \n",
    "    lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
    "        .setInputCols([column+\"_normalized\"]) \\\n",
    "        .setOutputCol(column+\"_lemma\")\n",
    "   \n",
    "    stopwords_cleaner = StopWordsCleaner()\\\n",
    "        .setInputCols(column+\"_lemma\")\\\n",
    "        .setOutputCol(column+\"_cleanTokens\")\\\n",
    "        .setCaseSensitive(False)\n",
    "    \n",
    "    bert_embeddings = BertEmbeddings\\\n",
    "     .pretrained('bert_base_cased', 'en') \\\n",
    "     .setInputCols([column+\"_document\",column+\"_cleanTokens\"])\\\n",
    "     .setOutputCol(column+\"_bert\")\\\n",
    "     .setCaseSensitive(False)\\\n",
    "     .setPoolingLayer(0)\n",
    "\n",
    "    embeddingsSentence = SentenceEmbeddings() \\\n",
    "          .setInputCols([column+\"_document\", column+\"_bert\"]) \\\n",
    "          .setOutputCol(column+\"_sentence_embeddings\") \\\n",
    "          .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "    embeddings_finisher = EmbeddingsFinisher() \\\n",
    "        .setInputCols([column+\"_sentence_embeddings\"]) \\\n",
    "        .setOutputCols([column+\"_finished_sentence_embeddings\"]) \\\n",
    "        .setOutputAsVector(True)\\\n",
    "        .setCleanAnnotations(True)\n",
    "\n",
    "    return [document_assembler, sentence_detector, tokenizer, spell_checker, normalizer, lemma, \\\n",
    "            stopwords_cleaner, bert_embeddings, embeddingsSentence, embeddings_finisher]\n",
    "\n",
    "def bert_pipeline():\n",
    "     \n",
    "    q1_stages = bert_partial(\"question1\")\n",
    "    \n",
    "    q2_stages = bert_partial(\"question2\")\n",
    "    \n",
    "    label_stringIdx = StringIndexer(inputCol = \"is_duplicate\", outputCol = \"label\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=q1_stages+q2_stages+[label_stringIdx])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.2 MB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "bert_base_cased download started this may take some time.\n",
      "Approximate size to download 389.2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "nlp_pipeline_bert = bert_pipeline()\n",
    "\n",
    "nlp_model_bert = nlp_pipeline_bert.fit(df_limited)\n",
    "\n",
    "processed_bert = nlp_model_bert.transform(df_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------------------+--------------------+------------+--------------------------------------+--------------------------------------+-----+\n",
      "| id|qid1|qid2|           question1|           question2|is_duplicate|question1_finished_sentence_embeddings|question2_finished_sentence_embeddings|label|\n",
      "+---+----+----+--------------------+--------------------+------------+--------------------------------------+--------------------------------------+-----+\n",
      "|  0|   1|   2|What is the step ...|What is the step ...|           0|                  [[0.1943234652280...|                  [[0.2024581581354...|  0.0|\n",
      "|  1|   3|   4|What is the story...|What would happen...|           0|                  [[-0.479584932327...|                  [[0.0168425478041...|  0.0|\n",
      "|  2|   5|   6|How can I increas...|How can Internet ...|           0|                  [[0.3350400924682...|                  [[-0.360031187534...|  0.0|\n",
      "|  3|   7|   8|Why am I mentally...|Find the remainde...|           0|                  [[0.0855144485831...|                  [[-0.650472879409...|  0.0|\n",
      "|  4|   9|  10|Which one dissolv...|Which fish would ...|           0|                  [[0.1552668064832...|                  [[-0.541451036930...|  0.0|\n",
      "+---+----+----+--------------------+--------------------+------------+--------------------------------------+--------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_bert.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "processed_bert2 = processed_bert.withColumn(\"q1_features\", explode(processed_bert.question1_finished_sentence_embeddings))\n",
    "processed_bert3 = processed_bert2.withColumn(\"q2_features\", explode(processed_bert2.question2_finished_sentence_embeddings))\n",
    "\n",
    "bert_final = processed_bert3.select('question1','question2','q1_features','q2_features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           question1|           question2|         q1_features|         q2_features|label|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|What is the step ...|What is the step ...|[0.19432346522808...|[0.20245815813541...|  0.0|\n",
      "|What is the story...|What would happen...|[-0.4795849323272...|[0.01684254780411...|  0.0|\n",
      "|How can I increas...|How can Internet ...|[0.33504009246826...|[-0.3600311875343...|  0.0|\n",
      "|Why am I mentally...|Find the remainde...|[0.08551444858312...|[-0.6504728794097...|  0.0|\n",
      "|Which one dissolv...|Which fish would ...|[0.15526680648326...|[-0.5414510369300...|  0.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparknlp",
   "language": "python",
   "name": "sparknlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
