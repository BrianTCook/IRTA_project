{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import sparknlp # nlp processing\n",
    "from sklearn.model_selection import train_test_split # splitting data\n",
    "import keras\n",
    "import matplotlib.pyplot as plt # visualisation\n",
    "import seaborn as sns # visualisation \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomState = np.random.RandomState(seed=42) # for creating same randomness in each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  2.5.0\n",
      "Apache Spark version:  2.4.5\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"questionBertEmbeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+--------------------------------------+--------------------------------------+-----+\n",
      "|    id|  qid1|  qid2|question1_finished_sentence_embeddings|question2_finished_sentence_embeddings|label|\n",
      "+------+------+------+--------------------------------------+--------------------------------------+-----+\n",
      "|119994|194709|194710|                  [[0.3533832728862...|                  [[0.1582194119691...|  1.0|\n",
      "|119995|194711|171467|                  [[0.3274492025375...|                  [[0.8858540058135...|  0.0|\n",
      "|119996|194712|194713|                  [[-0.307045459747...|                  [[0.4329249262809...|  0.0|\n",
      "|119997|111655| 64454|                  [[-0.437017947435...|                  [[-0.389505982398...|  0.0|\n",
      "|119998| 67996|194714|                  [[0.6548286676406...|                  [[0.4485912322998...|  1.0|\n",
      "+------+------+------+--------------------------------------+--------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- qid1: long (nullable = true)\n",
      " |-- qid2: long (nullable = true)\n",
      " |-- question1_finished_sentence_embeddings: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      " |-- question2_finished_sentence_embeddings: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df.withColumnRenamed(\"question1_finished_sentence_embeddings\",\"features1\")\\\n",
    ".withColumnRenamed(\"question2_finished_sentence_embeddings\",\"features2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- qid1: long (nullable = true)\n",
      " |-- qid2: long (nullable = true)\n",
      " |-- features1: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      " |-- features2: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_renamed.randomSplit([.8, .2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(df, chunks=10):\n",
    "    x_train_1 = []\n",
    "    x_train_2 = []\n",
    "    y_train = []\n",
    "\n",
    "    row_count = df.count()\n",
    "    i = 0\n",
    "    \n",
    "    chunks = df.randomSplit(weights=[1/chunks] * chunks)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        rows = chunk.collect()\n",
    "        for row in rows:\n",
    "            if i % 100000 == 0:\n",
    "                print('row {} / {} ({:.1f} %)'.format(i, row_count, 100 * i / row_count))\n",
    "            f1 = np.array(row['features1'][0]).reshape(-1,1)\n",
    "            f2 = np.array(row['features2'][0]).reshape(-1,1)\n",
    "            label = row['label']\n",
    "            x_train_1.append(f1)\n",
    "            x_train_2.append(f2)\n",
    "            y_train.append(label)\n",
    "            i += 1\n",
    "\n",
    "    #x_train = np.array([np.array(x_train_1), np.array(x_train_2)])\n",
    "    y_train = np.array(y_train)\n",
    "    return x_train_1, x_train_2, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0 / 323344 (0.0 %)\n",
      "row 100000 / 323344 (30.9 %)\n",
      "row 200000 / 323344 (61.9 %)\n",
      "row 300000 / 323344 (92.8 %)\n"
     ]
    }
   ],
   "source": [
    "x_train_1, x_train_2, y_train = build_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = sequence.pad_sequences(x_train_1, maxlen=768)\n",
    "x_train_2 = sequence.pad_sequences(x_train_2, maxlen=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0 / 80943 (0.0 %)\n"
     ]
    }
   ],
   "source": [
    "x_test_1, x_test_2, y_test = build_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_1 = sequence.pad_sequences(x_test_1, maxlen=768)\n",
    "x_test_2 = sequence.pad_sequences(x_test_2, maxlen=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323344, 768, 1) (323344, 768, 1)\n",
      "(80943, 768, 1) (80943, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_1.shape, x_train_2.shape)\n",
    "print(x_test_1.shape, x_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels:\n",
      " 0.0    204157\n",
      "1.0    119187\n",
      "dtype: int64\n",
      "Test Labels:\n",
      " 0.0    50867\n",
      "1.0    30076\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Train Labels:\\n', pd.Series(y_train).value_counts())\n",
    "print('Test Labels:\\n', pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, Dot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese Approach (Distance Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    _input = Input(shape=input_shape)\n",
    "    x = Flatten()(_input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, 'float64')))\n",
    "\n",
    "def absolute_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"opencl_amd_radeon_pro_5500m_compute_engine.0\"\n"
     ]
    }
   ],
   "source": [
    "# network definition\n",
    "base_network = create_base_network((768, 1))\n",
    "\n",
    "input_a = Input(shape=(768, 1))\n",
    "input_b = Input(shape=(768, 1))\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model([input_a, input_b], distance)\n",
    "rms = RMSprop()\n",
    "model.compile(loss='binary_crossentropy', optimizer=rms, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323344 samples, validate on 80943 samples\n",
      "Epoch 1/5\n",
      "323344/323344 [==============================] - 24s 74us/step - loss: 0.6658 - acc: 0.6543 - val_loss: 0.8827 - val_acc: 0.6556\n",
      "Epoch 2/5\n",
      "323344/323344 [==============================] - 23s 71us/step - loss: 0.6905 - acc: 0.6637 - val_loss: 0.9173 - val_acc: 0.6502\n",
      "Epoch 3/5\n",
      "323344/323344 [==============================] - 23s 71us/step - loss: 0.7310 - acc: 0.6582 - val_loss: 0.9943 - val_acc: 0.6445\n",
      "Epoch 4/5\n",
      "323344/323344 [==============================] - 23s 72us/step - loss: 0.7681 - acc: 0.6531 - val_loss: 1.0502 - val_acc: 0.6380\n",
      "Epoch 5/5\n",
      "323344/323344 [==============================] - 24s 73us/step - loss: 0.7868 - acc: 0.6506 - val_loss: 1.0503 - val_acc: 0.6437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13f4f6510>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_1, x_train_2], y_train,\n",
    "          batch_size=128,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test_1, x_test_2], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([x_test_1, x_test_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = np.array([0.0 if x<0.5 else 1.0 for x in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.97      0.79     50867\n",
      "         1.0       0.80      0.20      0.32     30076\n",
      "\n",
      "    accuracy                           0.68     80943\n",
      "   macro avg       0.73      0.59      0.56     80943\n",
      "weighted avg       0.72      0.68      0.62     80943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras dot layer provides l2 normalized dot production, which gives cosine proximity\n",
    "out = Dot(axes=1, normalize=True)([processed_a, processed_b])\n",
    "\n",
    "model = Model([input_a, input_b], out)\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323344 samples, validate on 80943 samples\n",
      "Epoch 1/5\n",
      "323344/323344 [==============================] - 31s 96us/step - loss: 5.9413 - accuracy: 0.6314 - val_loss: 5.9890 - val_accuracy: 0.6284\n",
      "Epoch 2/5\n",
      "323344/323344 [==============================] - 27s 82us/step - loss: 5.9413 - accuracy: 0.6314 - val_loss: 5.9890 - val_accuracy: 0.6284\n",
      "Epoch 3/5\n",
      "323344/323344 [==============================] - 27s 83us/step - loss: 5.9413 - accuracy: 0.6314 - val_loss: 5.9890 - val_accuracy: 0.6284\n",
      "Epoch 4/5\n",
      "323344/323344 [==============================] - 27s 82us/step - loss: 5.9413 - accuracy: 0.6314 - val_loss: 5.9890 - val_accuracy: 0.6284\n",
      "Epoch 5/5\n",
      "323344/323344 [==============================] - 27s 83us/step - loss: 5.9413 - accuracy: 0.6314 - val_loss: 5.9890 - val_accuracy: 0.6284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a4a68d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_1, x_train_2], y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=([x_test_1, x_test_2], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00     50867\n",
      "         1.0       0.37      1.00      0.54     30076\n",
      "\n",
      "    accuracy                           0.37     80943\n",
      "   macro avg       0.19      0.50      0.27     80943\n",
      "weighted avg       0.14      0.37      0.20     80943\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muradbozik/.pyenv/versions/3.7.7/envs/sparknlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([x_test_1, x_test_2])\n",
    "y_p = np.array([0.0 if x<0.5 else 1.0 for x in y_pred])\n",
    "print(classification_report(y_test, y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import recurrent, concatenate, Embedding\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "HIDDEN_SIZE = 50\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = Input(shape=(768,1))\n",
    "encoded_question1 = RNN(HIDDEN_SIZE)(question1)\n",
    "\n",
    "question2 = Input(shape=(768,1))\n",
    "encoded_question2 = RNN(HIDDEN_SIZE)(question2)\n",
    "\n",
    "merged = concatenate([encoded_question1, encoded_question2])\n",
    "preds = Dense(1, activation='softmax')(merged)\n",
    "\n",
    "model = Model([question1, question2], preds)\n",
    "rms = RMSprop()\n",
    "model.compile(optimizer=rms,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([x_train_1, x_train_2], y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=([x_test_1, x_test_2], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.optimizers import Nadam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_modelBase(num_units, dropout_rate=0.2, activation='relu'):\n",
    "    return models.Sequential([\n",
    "        layers.Dense(num_units, activation=activation, use_bias=True),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_units, activation=activation, use_bias=True),\n",
    "        layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "\n",
    "def normalizer(axis):\n",
    "    def _normalize(att_weights):\n",
    "        exp_weights = K.exp(att_weights)\n",
    "        sum_weights = K.sum(exp_weights, axis=axis, keepdims=True)\n",
    "        return exp_weights/sum_weights\n",
    "    return _normalize\n",
    "\n",
    "# function to sum a vector\n",
    "def sum_word(x):\n",
    "    return K.sum(x, axis=1) \n",
    "\n",
    "def build_model(max_length=250, num_hidden=200, dropout_rate=0.2, learn_rate=0.0001, optimizer='rmsprop'):\n",
    "  \n",
    "    # clear Keras session to free up GPU memory\n",
    "    K.clear_session()\n",
    "    \n",
    "    # input_a -> question1\n",
    "    # input_b -> question2\n",
    "    input_a = layers.Input(shape=(768,1))\n",
    "    input_b = layers.Input(shape=(768,1))\n",
    "    \n",
    "    ## step 2: encode\n",
    "    # compute attention weights\n",
    "    Q1 = build_modelBase(num_hidden, dropout_rate=dropout_rate)\n",
    "    \n",
    "    ## step 3: attend \n",
    "    # combine the soft-aligned vectors with the corresponding word vectors \n",
    "    Q2 = build_modelBase(num_hidden, dropout_rate=dropout_rate)   \n",
    "    \n",
    "    a = Q1(input_a)\n",
    "    b = Q2(input_b)\n",
    "    \n",
    "    #att = tf.keras.layers.Attention()([a, b])\n",
    "    att_weights = layers.dot([a, b], axes=-1, normalize=True) \n",
    "    \n",
    "    # normalize the attention weights\n",
    "    norm_weights_a = layers.Lambda(normalizer(1))(att_weights)\n",
    "    norm_weights_b = layers.Lambda(normalizer(2))(att_weights)\n",
    "    \n",
    "    # compute version of question a that is soft-aligned with every word of b\n",
    "    alpha = layers.dot([norm_weights_a, a], axes=1)\n",
    "    # compute version of question b that is soft-aligned with every word of a\n",
    "    beta  = layers.dot([norm_weights_b, b], axes=1)\n",
    "    \n",
    "    comp1 = layers.concatenate([a, beta])\n",
    "    comp2 = layers.concatenate([b, alpha])\n",
    "    \n",
    "    G = build_modelBase(num_hidden, dropout_rate=dropout_rate)  \n",
    "    v1 = layers.TimeDistributed(G)(comp1)\n",
    "    v2 = layers.TimeDistributed(G)(comp2)\n",
    "    \n",
    "    # and reduce the vectors computed above to a single vector per question\n",
    "    v1_sum = layers.Lambda(sum_word)(v1)\n",
    "    v2_sum = layers.Lambda(sum_word)(v2)\n",
    "    \n",
    "    concat = layers.concatenate([v1_sum, v2_sum])\n",
    "        \n",
    "    ## step 4: predict \n",
    "    H = build_modelBase(num_hidden, dropout_rate=dropout_rate)\n",
    "    out = H(concat)\n",
    "    out = layers.Dense(1, activation='sigmoid', use_bias=True)(out)\n",
    "    \n",
    "    # optimizer for gradient descent\n",
    "    model = Model([input_a, input_b], out)\n",
    "    if optimizer == 'sgd':\n",
    "        opt = SGD(lr=learn_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        opt = Adam(lr=learn_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(lr=learn_rate)\n",
    "    else:\n",
    "        opt = Nadam(lr=learn_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"opencl_amd_radeon_pro_5500m_compute_engine.0\"\n",
      "INFO:plaidml:Opening device \"opencl_amd_radeon_pro_5500m_compute_engine.0\"\n",
      "/Users/muradbozik/.pyenv/versions/3.7.7/envs/sparknlp/lib/python3.7/site-packages/keras/layers/core.py:665: UserWarning: `output_shape` argument not specified for layer lambda_31 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 768, 768)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/Users/muradbozik/.pyenv/versions/3.7.7/envs/sparknlp/lib/python3.7/site-packages/keras/layers/core.py:665: UserWarning: `output_shape` argument not specified for layer lambda_32 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 768, 768)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/Users/muradbozik/.pyenv/versions/3.7.7/envs/sparknlp/lib/python3.7/site-packages/keras/layers/core.py:665: UserWarning: `output_shape` argument not specified for layer lambda_33 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 768, 200)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/Users/muradbozik/.pyenv/versions/3.7.7/envs/sparknlp/lib/python3.7/site-packages/keras/layers/core.py:665: UserWarning: `output_shape` argument not specified for layer lambda_34 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 768, 200)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([x_train_1, x_train_2], y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparknlp",
   "language": "python",
   "name": "sparknlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
