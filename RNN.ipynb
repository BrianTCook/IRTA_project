{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import sparknlp # nlp processing\n",
    "from sklearn.model_selection import train_test_split # splitting data\n",
    "\n",
    "import matplotlib.pyplot as plt # visualisation\n",
    "import seaborn as sns # visualisation \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  2.5.0\n",
      "Apache Spark version:  2.4.5\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset/clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing + Bert Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_partial(column):\n",
    "    document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(column) \\\n",
    "        .setOutputCol(column+\"_document\")\\\n",
    "        .setCleanupMode(\"shrink\") \n",
    "    \n",
    "    sentence_detector = SentenceDetector() \\\n",
    "        .setInputCols([column+\"_document\"]) \\\n",
    "        .setOutputCol(column+\"_sentence\") \\\n",
    "        .setUseAbbreviations(True)\n",
    "    \n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([column+\"_sentence\"]) \\\n",
    "        .setOutputCol(column+\"_token\")\n",
    "    \n",
    "    spell_checker = NorvigSweetingApproach() \\\n",
    "        .setInputCols([column+\"_token\"]) \\\n",
    "        .setOutputCol(column+\"_checked\") \\\n",
    "        .setDictionary(\"./spell/coca2017.txt\", \"[a-zA-Z]+\")\n",
    "    \n",
    "    normalizer = Normalizer() \\\n",
    "        .setInputCols([column+\"_checked\"]) \\\n",
    "        .setOutputCol(column+\"_normalized\")\n",
    "    \n",
    "    lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
    "        .setInputCols([column+\"_normalized\"]) \\\n",
    "        .setOutputCol(column+\"_lemma\")\n",
    "   \n",
    "    stopwords_cleaner = StopWordsCleaner()\\\n",
    "        .setInputCols(column+\"_lemma\")\\\n",
    "        .setOutputCol(column+\"_cleanTokens\")\\\n",
    "        .setCaseSensitive(False)\n",
    "    \n",
    "    finisher = Finisher() \\\n",
    "        .setInputCols([column+\"_cleanTokens\"]) \\\n",
    "        .setOutputCols([column+\"_finished\"])\\\n",
    "        .setIncludeMetadata(False)\\\n",
    "        .setCleanAnnotations(True)\n",
    "\n",
    "    return [document_assembler, sentence_detector, tokenizer, spell_checker, normalizer, lemma, stopwords_cleaner, finisher]\n",
    "\n",
    "def preprocessing_pipeline():\n",
    "     \n",
    "    q1_stages = preprocess_partial(\"question1\")\n",
    "    \n",
    "    q2_stages = preprocess_partial(\"question2\")\n",
    "    \n",
    "    label_stringIdx = StringIndexer(inputCol = \"is_duplicate\", outputCol = \"label\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=q1_stages+q2_stages+[label_stringIdx])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "nlp_pipeline = preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    for ix, df_part in enumerate(np.array_split(data, 202)):\n",
    "        chunk = sql.createDataFrame(df_part) \n",
    "        nlp_model = nlp_pipeline.fit(chunk)\n",
    "        result = nlp_model.transform(chunk)\n",
    "\n",
    "        if ix == 0:\n",
    "            result_df = result\n",
    "        else:\n",
    "            result_df = result_df.unionAll(result)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------------------+--------------------+------------+--------------------+--------------------+-----+\n",
      "| id|qid1|qid2|           question1|           question2|is_duplicate|  question1_finished|  question2_finished|label|\n",
      "+---+----+----+--------------------+--------------------+------------+--------------------+--------------------+-----+\n",
      "|  0|   1|   2|What is the step ...|What is the step ...|           0|[step, step, guid...|[step, step, guid...|  0.0|\n",
      "+---+----+----+--------------------+--------------------+------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ML ClassifierDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import feature as spark_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(col):\n",
    "    return spark_ft.Word2Vec(vectorSize=50, minCount=2, seed=123, \n",
    "                             inputCol=f'question{col}_finished', outputCol=f'q{col}_text_vec', \n",
    "                             windowSize=5, maxSentenceLength=250)\n",
    "\n",
    "text2vec1 = text2vec(1)\n",
    "text2vec2 = text2vec(2)\n",
    "\n",
    "# assembler = spark_ft.VectorAssembler(inputCols=['q1_text_vec', 'q2_text_vec'], outputCol='features')\n",
    "\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "  .setInputCols(['q1_text_vec', 'q2_text_vec'])\\\n",
    "  .setOutputCol(\"preds\")\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setMaxEpochs(20)\\\n",
    "  .setEnableOutputLogs(True)\n",
    "\n",
    "feature_pipeline = Pipeline(stages=[text2vec1, text2vec2, classsifierdl])\n",
    "feature_model = feature_pipeline.fit(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/annotator_logs/ClassifierDLApproach_d4a8d8ae15c4.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_df = feature_model.transform(processed_df).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[-0.3750478071825...|  0.0|\n",
      "|[-0.1569854863919...|  0.0|\n",
      "|[-0.2768606580793...|  0.0|\n",
      "|[0.10770221551259...|  0.0|\n",
      "|[0.17948742415755...|  0.0|\n",
      "|[0.00731686313520...|  1.0|\n",
      "|[-0.1786983711645...|  0.0|\n",
      "|[-0.1171621502144...|  1.0|\n",
      "|[-0.0333446813747...|  0.0|\n",
      "|[-0.1219908632338...|  0.0|\n",
      "|[-0.1354868883666...|  0.0|\n",
      "|[-0.7762437500059...|  1.0|\n",
      "|[-0.3195876628160...|  1.0|\n",
      "|[0.02170692756772...|  1.0|\n",
      "|[-0.0161319741358...|  0.0|\n",
      "|[-0.0455943652325...|  1.0|\n",
      "|[0.17729056905955...|  1.0|\n",
      "|[-0.0264204561710...|  0.0|\n",
      "|[-0.3555452488362...|  1.0|\n",
      "|[-0.5042165249586...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized_clean_df = featurized_df.select('features', 'label')\n",
    "featurized_clean_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec1 = text2vec(1)\n",
    "text2vec2 = text2vec(2)\n",
    "assembler = spark_ft.VectorAssembler(inputCols=['q1_text_vec', 'q2_text_vec'], outputCol='features')\n",
    "feature_pipeline = Pipeline(stages=[text2vec1, text2vec2, assembler])\n",
    "feature_model = feature_pipeline.fit(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_df = feature_model.transform(processed_df).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = featurized_df.randomSplit([.8, .2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+---------------------+--------------------+------------+----------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "| id|qid1|qid2|            question1|           question2|is_duplicate|    question1_finished|  question2_finished|label|         q1_text_vec|         q2_text_vec|            features|\n",
      "+---+----+----+---------------------+--------------------+------------+----------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|  3|   7|   8| Why am I mentally...|Find the remainde...|           0|  [mentally, lonely...|[wind, remainder,...|  0.0|[0.10770221551259...|[0.08770019956864...|[0.10770221551259...|\n",
      "|  4|   9|  10| Which one dissolv...|Which fish would ...|           0|  [one, dissolve, w...|[fish, survive, s...|  0.0|[0.17948742415755...|[0.21307177469134...|[0.17948742415755...|\n",
      "|  5|  11|  12| Astrology: I am a...|I'm a triple Capr...|           1|  [astrology, capri...|[Im, triple, capr...|  1.0|[0.00731686313520...|[0.07671441232923...|[0.00731686313520...|\n",
      "|  6|  13|  14|  Should I buy tiago?|What keeps childe...|           0|          [buy, tiago]|[keep, childern, ...|  0.0|[-0.1786983711645...|[-0.1329361721873...|[-0.1786983711645...|\n",
      "|  7|  15|  16| How can I be a go...|What should I do ...|           1|     [good, geologist]|  [great, geologist]|  1.0|[-0.1171621502144...|[-0.1262290081940...|[-0.1171621502144...|\n",
      "|  8|  17|  18|When do you use シ...|When do you use \"...|           0|[use, シ, instead, し]|      [use, instead]|  0.0|[-0.0333446813747...|[-0.0455584749579...|[-0.0333446813747...|\n",
      "|  9|  19|  20| Motorola (company...|How do I hack Mot...|           0|  [motorola, compan...|[hack, motorola, ...|  0.0|[-0.1219908632338...|[-0.1843010976910...|[-0.1219908632338...|\n",
      "| 10|  21|  22| Method to find se...|What are some of ...|           0|  [method, find, se...|[thing, technicia...|  0.0|[-0.1354868883666...|[-0.0282973195093...|[-0.1354868883666...|\n",
      "| 11|  23|  24| How do I read and...|How can I see all...|           1|  [read, find, YouT...|[see, youtube, co...|  1.0|[-0.7762437500059...|[-0.2419236128528...|[-0.7762437500059...|\n",
      "| 12|  25|  26| What can make Phy...|How can you make ...|           1|  [make, physics, e...|[make, physics, e...|  1.0|[-0.3195876628160...|[-0.3451644023880...|[-0.3195876628160...|\n",
      "| 14|  29|  30| What are the laws...|What are the laws...|           0|  [law, change, sta...|[law, change, sta...|  0.0|[-0.0161319741358...|[-0.1363421132555...|[-0.0161319741358...|\n",
      "| 15|  31|  32| What would a Trum...|How will a Trump ...|           1|  [trump, presidenc...|[trump, presidenc...|  1.0|[-0.0455943652325...|[-0.0160414360677...|[-0.0455943652325...|\n",
      "| 16|  33|  34| What does manipul...|What does manipul...|           1|  [manipulation, mean]|[manipulation, mean]|  1.0|[0.17729056905955...|[-0.0599610498175...|[0.17729056905955...|\n",
      "| 17|  35|  36| Why do girls want...|How do guys feel ...|           0|  [girl, want, frie...|[guy, feel, rejec...|  0.0|[-0.0264204561710...|[-0.0388346124673...|[-0.0264204561710...|\n",
      "| 18|  37|  38| Why are so many Q...|Why do people ask...|           1|  [many, quora, use...|[people, ask, quo...|  1.0|[-0.3555452488362...|[-0.0147477582629...|[-0.3555452488362...|\n",
      "| 19|  39|  40| Which is the best...|Which is the best...|           0|  [good, digital, m...|[good, digital, m...|  0.0|[-0.5042165249586...|[-0.4026117950677...|[-0.5042165249586...|\n",
      "| 20|  41|  42| Why do rockets lo...|Why are rockets a...|           1|  [rocket, look, wh...|[rocket, booster,...|  1.0|[0.25047093629837...|[0.12126135919243...|[0.25047093629837...|\n",
      "| 21|  43|  44| What's causing so...|What can I do to ...|           0|  [cause, someone, ...|[avoid, jealous, ...|  0.0|[0.19330619772275...|[-0.0607019215822...|[0.19330619772275...|\n",
      "| 22|  45|  46| What are the ques...|Which question sh...|           0|  [question, ask, q...|[question, ask, q...|  0.0|[-0.3199807504812...|[0.07862675686677...|[-0.3199807504812...|\n",
      "| 23|  47|  48| How much is 30 kV...|Where can I find ...|           0|        [much, kV, HP]|[find, conversion...|  0.0|[-0.0410964135080...|[-0.0977117024362...|[-0.0410964135080...|\n",
      "+---+----+----+---------------------+--------------------+------------+----------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, RNN\n",
    "\n",
    "## custom keras layer to implement the attention mechanism (with trainable weights) for the hierarchical attention model. \n",
    "# Implementation based on word and sentence attention layers described in Yang et al. and keras custom layer example\n",
    "class Attention_Layer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        super(Attention_Layer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create the trainable weight variables for this layer.\n",
    "        self.W = self.add_weight(name='W', \n",
    "                                      shape=(input_shape[-1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.b = self.add_weight(name='b', \n",
    "                                      shape=(self.output_dim,),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.u = self.add_weight(name='u', \n",
    "                                      shape=(self.output_dim,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Attention_Layer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, h_it):        \n",
    "        u_it = K.tanh(K.bias_add(K.dot(h_it, self.W), self.b))\n",
    "        att_weights = K.dot(u_it, self.u)\n",
    "        exp_weights = K.exp(att_weights)\n",
    "        sum_weights = K.sum(exp_weights, axis=1, keepdims=True)\n",
    "        alpha_it = exp_weights/sum_weights\n",
    "        return K.sum(h_it*alpha_it, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_model(vectors, optimizer, learn_rate, dropout_rate1, dropout_rate2, max_length=50, num_hidden=200, num_classes=1, \n",
    "                projected_dim=200):\n",
    "    K.clear_session()\n",
    "    \n",
    "    # input    \n",
    "    model_input = layers.Input(shape=(2, max_length), dtype='int32')\n",
    "    \n",
    "    # embeddings (projected)\n",
    "    embed = create_embedding(vectors, max_length, projected_dim)\n",
    "    \n",
    "    # step 1: word encoder\n",
    "    word_sequence_input = layers.Input(shape=(max_length,), dtype='int32')\n",
    "    h_w = layers.Bidirectional(layers.GRU(num_hidden, dropout=dropout_rate1, return_sequences=True))(embed(word_sequence_input))\n",
    "    \n",
    "    # step 2: word attention\n",
    "    s_w = Attention_Layer(num_hidden)(h_w)\n",
    "    word_encode_attend = Model(word_sequence_input, s_w)\n",
    "    \n",
    "    # step 3: question encoder\n",
    "    q_encode_attend = layers.TimeDistributed(word_encode_attend)(model_input)\n",
    "    h = layers.Bidirectional(layers.GRU(num_hidden, dropout=dropout_rate2, return_sequences=True))(q_encode_attend)\n",
    "    \n",
    "    # step 4: question attention\n",
    "    v = Attention_Layer(num_hidden)(h)\n",
    "    \n",
    "    # step 5: final classification\n",
    "    out = layers.Dense(num_classes, activation='sigmoid', use_bias=True)(v)\n",
    "    \n",
    "    model = Model(model_input, out)\n",
    "    \n",
    "    if optimizer == 'sgd':\n",
    "        opt = SGD(lr=learn_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        opt = Adam(lr=learn_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(lr=learn_rate)\n",
    "    else:\n",
    "        opt = Nadam(lr=learn_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(df, chunks=10):\n",
    "    x_1 = []\n",
    "    x_2 = []\n",
    "    y_train = []\n",
    "\n",
    "    row_count = df.count()\n",
    "    i = 0\n",
    "    \n",
    "    chunks = df.randomSplit(weights=[1/chunks] * chunks)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        rows = chunk.collect()\n",
    "        for row in rows:\n",
    "            if i % 100000 == 0:\n",
    "                print('row {} / {} ({:.1f} %)'.format(i, row_count, 100 * i / row_count))\n",
    "            label = row['label']\n",
    "            x_1.append(row['question1_finished'])\n",
    "            x_2.append(row['question2_finished'])\n",
    "            y_train.append(label)\n",
    "            i += 1\n",
    "\n",
    "    #x_train = np.array([np.array(x_train_1), np.array(x_train_2)])\n",
    "    y_train = np.array(y_train)\n",
    "    return x_1, x_2, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0 / 323543 (0.0 %)\n",
      "row 100000 / 323543 (30.9 %)\n",
      "row 200000 / 323543 (61.8 %)\n",
      "row 300000 / 323543 (92.7 %)\n"
     ]
    }
   ],
   "source": [
    "q1, q2, yt = build_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_modelBase(num_units, dropout_rate=0.2, activation='relu'):\n",
    "    return models.Sequential([\n",
    "        layers.Dense(num_units, activation=activation, use_bias=True),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_units, activation=activation, use_bias=True),\n",
    "        layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "\n",
    "def normalizer(axis):\n",
    "    def _normalize(att_weights):\n",
    "        exp_weights = K.exp(att_weights)\n",
    "        sum_weights = K.sum(exp_weights, axis=axis, keepdims=True)\n",
    "        return exp_weights/sum_weights\n",
    "    return _normalize\n",
    "\n",
    "# function to sum a vector\n",
    "def sum_word(x):\n",
    "    return K.sum(x, axis=1) \n",
    "\n",
    "def build_model(questions, max_length=250, num_hidden=200, dropout_rate=0.2, learn_rate=0.0001):\n",
    "  \n",
    "    # clear Keras session to free up GPU memory\n",
    "    K.clear_session()\n",
    "    \n",
    "    # input_a -> question1\n",
    "    # input_b -> question2\n",
    "    input_a = layers.Input(shape=(768,1))\n",
    "    input_b = layers.Input(shape=(768,1))\n",
    "    \n",
    "    ## step 2: encode\n",
    "    # compute attention weights\n",
    "    Q1 = build_modelBase(num_hidden, dropout_rate=dropout_rate)\n",
    "    \n",
    "    ## step 3: attend \n",
    "    # combine the soft-aligned vectors with the corresponding word vectors \n",
    "    Q2 = build_modelBase(num_hidden, dropout_rate=dropout_rate)   \n",
    "    \n",
    "    a = Q1(input_a)\n",
    "    b = Q2(input_b)\n",
    "    \n",
    "    att = layers.Attention()([a, b])\n",
    "    \n",
    "    # normalize the attention weights\n",
    "    norm_weights_a = layers.Lambda(normalizer(1))(att)\n",
    "    norm_weights_b = layers.Lambda(normalizer(2))(att)\n",
    "    \n",
    "    # compute version of question a that is soft-aligned with every word of b\n",
    "    alpha = layers.dot([norm_weights_a, a], axes=1)\n",
    "    # compute version of question b that is soft-aligned with every word of a\n",
    "    beta  = layers.dot([norm_weights_b, b], axes=1)\n",
    "    \n",
    "    comp1 = layers.concatenate([a, beta])\n",
    "    comp2 = layers.concatenate([b, alpha])\n",
    "    \n",
    "    v1 = layers.TimeDistributed(G)(comp1)\n",
    "    v2 = layers.TimeDistributed(G)(comp2)\n",
    "    \n",
    "    # and reduce the vectors computed above to a single vector per question\n",
    "    v1_sum = layers.Lambda(sum_word)(v1)\n",
    "    v2_sum = layers.Lambda(sum_word)(v2)\n",
    "    \n",
    "    concat = layers.concatenate([v1_sum, v2_sum])\n",
    "        \n",
    "    ## step 4: predict \n",
    "    H = build_modelBase(num_hidden, dropout_rate=dropout_rate3)\n",
    "    out = H(concat)\n",
    "    out = layers.Dense(num_classes, activation='sigmoid', use_bias=True)(out)\n",
    "    \n",
    "    # optimizer for gradient descent\n",
    "    model = Model(model_input, out)\n",
    "    \n",
    "    opt = RMSprop(lr=learn_rate)\n",
    "   \n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "df = nlp_model_bert.transform(df_limited).select('label', 'question1', 'question2', 'class.result').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      1.00      0.77      1250\n",
      "         1.0       0.00      0.00      0.00       741\n",
      "         2.0       0.00      0.00      0.00         1\n",
      "         3.0       0.00      0.00      0.00         1\n",
      "         4.0       0.00      0.00      0.00         1\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "         6.0       0.00      0.00      0.00         1\n",
      "         7.0       0.00      0.00      0.00         1\n",
      "         8.0       0.00      0.00      0.00         1\n",
      "         9.0       0.00      0.00      0.00         1\n",
      "        10.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62      2000\n",
      "   macro avg       0.06      0.09      0.07      2000\n",
      "weighted avg       0.39      0.62      0.48      2000\n",
      "\n",
      "0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muradbozik/.pyenv/versions/3.7.7/envs/sparknlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df.label, df.result))\n",
    "print(accuracy_score(df.label, df.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>10.0</td>\n",
       "      <td>\"Is there a \"\"blind trust\"\" provision for Amer...</td>\n",
       "      <td>and how is it enforced?\"</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                          question1  \\\n",
       "813   10.0  \"Is there a \"\"blind trust\"\" provision for Amer...   \n",
       "\n",
       "                     question2  result  \n",
       "813   and how is it enforced?\"     0.0  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.label == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparknlp",
   "language": "python",
   "name": "sparknlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
