{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 XGBoost模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/quora-question-pairs-feature-extraction-2/train.csv\")\n",
    "test = pd.read_csv(\"../input/quora-question-pairs-feature-extraction-2/test.csv\")\n",
    "trainlabel = pd.read_csv(\"../input/quora-question-pairs-feature-extraction-2/trainlabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train, label = trainlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 训练集和测试集具有不同的分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "留意到训练集的样本分布于测试集的有所不同。\n",
    "\n",
    "训练集正例的比例是0.3691，将这个训练集均值作为对测试集的预测上传，Public Leaderboard上的得分为0.55410，Private Leaderboard上的得分为0.55525。根据对数损失的计算方法，可以逆推出Public的正例比例为0.174247，private的正例比例为0.176394。为了使得Public和Private榜上的得分尽可能接近，直接取两者平均，得0.175320，作为对测试集正例的估计，基于此对训练集进行适当调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17424682519173934 0.17639371112774202 0.17532026815974067\n"
     ]
    }
   ],
   "source": [
    "p = 0.369197853026293\n",
    "pos_public = (0.55410 + np.log(1 - p)) / np.log((1 - p) / p)\n",
    "pos_private = (0.55525 + np.log(1 - p)) / np.log((1 - p) / p)\n",
    "average = (pos_public + pos_private) / 2\n",
    "print (pos_public, pos_private, average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost有scale_pos_weight参数可用于调整正负样本的权重，很容易计算，要等价于让训练集正例的数量从0.3691下降至0.1753，应让正例的权重变为原来的0.3632。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3632292393086082\n"
     ]
    }
   ],
   "source": [
    "w0 = average * (1 - p) / ((1 - average) * p)\n",
    "print(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一方面，虽然训练时logloss的计算的却会根据scale_pos_weight有所调整，也就是说scale_pos_weight参数的确可以影响到XGBoost的训练过程，但对于验证过程，用于交叉验证的xgb.cv函数却没有这样的效果，该函数的交叉验证logloss仍然是根据正负例等权重的方法计算得到。因此，本文重新定义了一个加权对数损失函数，输入给xgb.cv函数的feval参数用于交叉验证的损失计算，而训练过程中的损失函数则无需自定义计算。\n",
    "\n",
    "要等价于让正例的数量从0.3691下降到0.1753，而正负例的总量维持不变，应对正例和负例分别施加0.4749的1.3074的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.474868059829305 1.3073508639700833\n"
     ]
    }
   ],
   "source": [
    "w1 = average / p\n",
    "w2 = (1 - average) / (1 - p)\n",
    "print(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_log_loss(preds, dtrain):\n",
    "    label = dtrain.get_label()\n",
    "    return \"weighted_logloss\", -np.mean(w1 * label * np.log(preds) + w2 * (1 - label) * np.log(1 - preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 XGBoost参数调整\n",
    "\n",
    "参数调整的流程如下：\n",
    "\n",
    "num_boost_round → max_depth & min_child_weight → max_bin(因为有GPU加速) → subsample & colsample_bytree → eta & num_boost_round\n",
    "\n",
    "## 3.2.1 调整基学习器数量num_boost_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.634759+7.02923e-05\ttrain-weighted_logloss:0.623622+9.99644e-05\ttest-logloss:0.634839+0.000291607\ttest-weighted_logloss:0.623696+0.00100518\n",
      "[50]\ttrain-logloss:0.321486+0.00043763\ttrain-weighted_logloss:0.20286+0.000321448\ttest-logloss:0.324138+0.00363903\ttest-weighted_logloss:0.20488+0.00219108\n",
      "[100]\ttrain-logloss:0.306676+0.000570014\ttrain-weighted_logloss:0.192694+0.000369073\ttest-logloss:0.312347+0.00363729\ttest-weighted_logloss:0.196903+0.00230343\n",
      "[150]\ttrain-logloss:0.298386+0.000598019\ttrain-weighted_logloss:0.187574+0.000366055\ttest-logloss:0.307222+0.00367447\ttest-weighted_logloss:0.194099+0.00229949\n",
      "[200]\ttrain-logloss:0.292377+0.000449283\ttrain-weighted_logloss:0.183811+0.000298574\ttest-logloss:0.30434+0.00365647\ttest-weighted_logloss:0.19263+0.00227717\n",
      "[250]\ttrain-logloss:0.287348+0.00042484\ttrain-weighted_logloss:0.180729+0.000254256\ttest-logloss:0.302486+0.00366283\ttest-weighted_logloss:0.191832+0.00230327\n",
      "[300]\ttrain-logloss:0.282891+0.000426437\ttrain-weighted_logloss:0.177953+0.000268319\ttest-logloss:0.301053+0.00368956\ttest-weighted_logloss:0.19123+0.00232084\n",
      "[350]\ttrain-logloss:0.278905+0.000407849\ttrain-weighted_logloss:0.175507+0.000238589\ttest-logloss:0.299923+0.00365094\ttest-weighted_logloss:0.190847+0.00230755\n",
      "[400]\ttrain-logloss:0.275243+0.000383455\ttrain-weighted_logloss:0.173246+0.000256443\ttest-logloss:0.299025+0.00361994\ttest-weighted_logloss:0.190574+0.00226753\n",
      "[450]\ttrain-logloss:0.271709+0.000421515\ttrain-weighted_logloss:0.171074+0.00027422\ttest-logloss:0.29822+0.00360595\ttest-weighted_logloss:0.190357+0.00226089\n",
      "[500]\ttrain-logloss:0.26839+0.000430899\ttrain-weighted_logloss:0.169019+0.000269716\ttest-logloss:0.297522+0.00356378\ttest-weighted_logloss:0.190171+0.00225108\n",
      "[550]\ttrain-logloss:0.265196+0.000335657\ttrain-weighted_logloss:0.167076+0.000219066\ttest-logloss:0.296981+0.00364973\ttest-weighted_logloss:0.190109+0.00227181\n",
      "[600]\ttrain-logloss:0.262046+0.000319265\ttrain-weighted_logloss:0.165129+0.000210372\ttest-logloss:0.296404+0.00362848\ttest-weighted_logloss:0.189989+0.00224751\n",
      "[650]\ttrain-logloss:0.258975+0.000363474\ttrain-weighted_logloss:0.163266+0.000262427\ttest-logloss:0.295882+0.00366717\ttest-weighted_logloss:0.189945+0.00227538\n",
      "[700]\ttrain-logloss:0.256041+0.000397305\ttrain-weighted_logloss:0.161469+0.00026018\ttest-logloss:0.295537+0.00365342\ttest-weighted_logloss:0.189993+0.00228553\n",
      "[750]\ttrain-logloss:0.25319+0.000402581\ttrain-weighted_logloss:0.159726+0.000271008\ttest-logloss:0.295186+0.00368279\ttest-weighted_logloss:0.190018+0.00229249\n",
      "[800]\ttrain-logloss:0.250479+0.00037466\ttrain-weighted_logloss:0.158048+0.000247301\ttest-logloss:0.29488+0.00367413\ttest-weighted_logloss:0.190068+0.00228135\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.1\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 1\n",
    "params[\"gamma\"] = 0\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"  # 使用GPU加速的直方图算法\n",
    "params['max_bin'] = 256\n",
    "\n",
    "model1 = xgb.cv(params, dtrain, num_boost_round = 2000, nfold = 10, \n",
    "                feval = weighted_log_loss, early_stopping_rounds = 200, \n",
    "                verbose_eval = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基学习器数量大约为650棵，因为验证集的加权对数损失test-weighted_logloss在650棵树以后出现了上升。\n",
    "\n",
    "## 3.2.2 调整最大深度max_depth和min_weight\n",
    "\n",
    "交叉验证通常可使用Scikit-Learn的GridSearchCV函数，但我在XGBoost的Scikit-Learn API参数页里没找到GPU相关选项，强行让tree_method=“gpu_hist”又报错，所以我怀疑GridSearchCV不支持GPU使用，被迫使用for循环，并用evaluation_depth_childweight记录下所有的交叉验证历史。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>497</th>\n",
       "      <th>472</th>\n",
       "      <th>508</th>\n",
       "      <th>379</th>\n",
       "      <th>356</th>\n",
       "      <th>391</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <td>0.299639</td>\n",
       "      <td>0.299818</td>\n",
       "      <td>0.300022</td>\n",
       "      <td>0.296271</td>\n",
       "      <td>0.296496</td>\n",
       "      <td>0.296799</td>\n",
       "      <td>0.295420</td>\n",
       "      <td>0.296186</td>\n",
       "      <td>0.295899</td>\n",
       "      <td>0.294930</td>\n",
       "      <td>0.295734</td>\n",
       "      <td>0.295567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-logloss-std</th>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.002430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-mean</th>\n",
       "      <td>0.190885</td>\n",
       "      <td>0.190973</td>\n",
       "      <td>0.190908</td>\n",
       "      <td>0.190444</td>\n",
       "      <td>0.190283</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.190193</td>\n",
       "      <td>0.190228</td>\n",
       "      <td>0.190176</td>\n",
       "      <td>0.190352</td>\n",
       "      <td>0.190338</td>\n",
       "      <td>0.190404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-std</th>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.001553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <td>0.276720</td>\n",
       "      <td>0.278095</td>\n",
       "      <td>0.279224</td>\n",
       "      <td>0.256770</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>0.263115</td>\n",
       "      <td>0.255204</td>\n",
       "      <td>0.259223</td>\n",
       "      <td>0.258160</td>\n",
       "      <td>0.249334</td>\n",
       "      <td>0.255266</td>\n",
       "      <td>0.253585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-std</th>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-mean</th>\n",
       "      <td>0.174246</td>\n",
       "      <td>0.175090</td>\n",
       "      <td>0.175791</td>\n",
       "      <td>0.161924</td>\n",
       "      <td>0.164172</td>\n",
       "      <td>0.165781</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>0.163346</td>\n",
       "      <td>0.162705</td>\n",
       "      <td>0.157218</td>\n",
       "      <td>0.160877</td>\n",
       "      <td>0.159889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-std</th>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  649       649    ...          356       391\n",
       "test-logloss-mean            0.299639  0.299818    ...     0.295734  0.295567\n",
       "test-logloss-std             0.002488  0.002403    ...     0.002368  0.002430\n",
       "test-weighted_logloss-mean   0.190885  0.190973    ...     0.190338  0.190404\n",
       "test-weighted_logloss-std    0.001524  0.001480    ...     0.001384  0.001553\n",
       "train-logloss-mean           0.276720  0.278095    ...     0.255266  0.253585\n",
       "train-logloss-std            0.000462  0.000314    ...     0.000351  0.000355\n",
       "train-weighted_logloss-mean  0.174246  0.175090    ...     0.160877  0.159889\n",
       "train-weighted_logloss-std   0.000305  0.000239    ...     0.000214  0.000213\n",
       "\n",
       "[8 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"subsample\"] = 0.8\n",
    "fix_params[\"colsample_bytree\"] = 0.9\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_bin\"] = 256\n",
    "\n",
    "evaluation_list = []\n",
    "for depth in [5, 6]:\n",
    "    for child_weight in [1, 2.5, 4]:\n",
    "        params = {**fix_params, **{\"max_depth\": depth, \"min_child_weight\": child_weight}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        # evaluation记录了每一轮迭代的交叉验证结果\n",
    "        evaluation_list.append(evaluation)\n",
    "        \n",
    "for depth in [7, 8]:\n",
    "    for child_weight in [4, 5, 6]:\n",
    "        params = {**fix_params, **{\"max_depth\": depth, \"min_child_weight\": child_weight}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        # evaluation记录了每一轮迭代的交叉验证结果\n",
    "        evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    # evaluation的最后一行即相应参数组合的结果\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重点观察上表第3行test-weighted_logloss-mean，代表了在验证集上的加权对数损失。第7列至第12列的模型因为早停机制在650棵树前提前停止训练，对比前6列结果可知，第6列最优，对数损失为0.190239，代表了max_depth=6, min_child_weight=4的组合。围绕这个组合继续构建搜索范围。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>476</th>\n",
       "      <th>500</th>\n",
       "      <th>497</th>\n",
       "      <th>466</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <td>0.299894</td>\n",
       "      <td>0.299870</td>\n",
       "      <td>0.300022</td>\n",
       "      <td>0.300072</td>\n",
       "      <td>0.296689</td>\n",
       "      <td>0.296817</td>\n",
       "      <td>0.296799</td>\n",
       "      <td>0.296996</td>\n",
       "      <td>0.295527</td>\n",
       "      <td>0.295577</td>\n",
       "      <td>0.295420</td>\n",
       "      <td>0.296260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-logloss-std</th>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.002482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-mean</th>\n",
       "      <td>0.190945</td>\n",
       "      <td>0.190832</td>\n",
       "      <td>0.190908</td>\n",
       "      <td>0.190979</td>\n",
       "      <td>0.190395</td>\n",
       "      <td>0.190328</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.190402</td>\n",
       "      <td>0.190186</td>\n",
       "      <td>0.190248</td>\n",
       "      <td>0.190193</td>\n",
       "      <td>0.190274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-std</th>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.001528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <td>0.278593</td>\n",
       "      <td>0.279125</td>\n",
       "      <td>0.279224</td>\n",
       "      <td>0.279594</td>\n",
       "      <td>0.261485</td>\n",
       "      <td>0.262322</td>\n",
       "      <td>0.263115</td>\n",
       "      <td>0.263708</td>\n",
       "      <td>0.254881</td>\n",
       "      <td>0.254063</td>\n",
       "      <td>0.255204</td>\n",
       "      <td>0.258704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-std</th>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-mean</th>\n",
       "      <td>0.175409</td>\n",
       "      <td>0.175740</td>\n",
       "      <td>0.175791</td>\n",
       "      <td>0.176036</td>\n",
       "      <td>0.164782</td>\n",
       "      <td>0.165298</td>\n",
       "      <td>0.165781</td>\n",
       "      <td>0.166155</td>\n",
       "      <td>0.160677</td>\n",
       "      <td>0.160155</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>0.163013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-std</th>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  649       649    ...          497       466\n",
       "test-logloss-mean            0.299894  0.299870    ...     0.295420  0.296260\n",
       "test-logloss-std             0.002428  0.002399    ...     0.002456  0.002482\n",
       "test-weighted_logloss-mean   0.190945  0.190832    ...     0.190193  0.190274\n",
       "test-weighted_logloss-std    0.001498  0.001491    ...     0.001520  0.001528\n",
       "train-logloss-mean           0.278593  0.279125    ...     0.255204  0.258704\n",
       "train-logloss-std            0.000579  0.000397    ...     0.000602  0.000683\n",
       "train-weighted_logloss-mean  0.175409  0.175740    ...     0.160860  0.163013\n",
       "train-weighted_logloss-std   0.000386  0.000275    ...     0.000400  0.000436\n",
       "\n",
       "[8 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"subsample\"] = 0.8\n",
    "fix_params[\"colsample_bytree\"] = 0.9\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_bin\"] = 256\n",
    "\n",
    "evaluation_list = []\n",
    "for depth in [5, 6, 7]:\n",
    "    for child_weight in [3, 3.5, 4, 4.5]:\n",
    "        params = {**fix_params, **{\"max_depth\": depth, \"min_child_weight\": child_weight}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上表可见，第7列最优，max_depth=6, min_child_weight=4的组合的确表现最好。\n",
    "\n",
    "## 3.2.3 调整直方图最大箱子数max_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <td>0.296946</td>\n",
       "      <td>0.296899</td>\n",
       "      <td>0.296799</td>\n",
       "      <td>0.296869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-logloss-std</th>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-mean</th>\n",
       "      <td>0.190354</td>\n",
       "      <td>0.190357</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.190378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-std</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <td>0.263007</td>\n",
       "      <td>0.263045</td>\n",
       "      <td>0.263115</td>\n",
       "      <td>0.262703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-std</th>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-mean</th>\n",
       "      <td>0.165729</td>\n",
       "      <td>0.165734</td>\n",
       "      <td>0.165781</td>\n",
       "      <td>0.165536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-std</th>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  649       649       649       649\n",
       "test-logloss-mean            0.296946  0.296899  0.296799  0.296869\n",
       "test-logloss-std             0.002580  0.002513  0.002576  0.002415\n",
       "test-weighted_logloss-mean   0.190354  0.190357  0.190239  0.190378\n",
       "test-weighted_logloss-std    0.001667  0.001584  0.001696  0.001499\n",
       "train-logloss-mean           0.263007  0.263045  0.263115  0.262703\n",
       "train-logloss-std            0.000594  0.000612  0.000503  0.000525\n",
       "train-weighted_logloss-mean  0.165729  0.165734  0.165781  0.165536\n",
       "train-weighted_logloss-std   0.000413  0.000410  0.000339  0.000332"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"subsample\"] = 0.8\n",
    "fix_params[\"colsample_bytree\"] = 0.9\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_depth\"] = 6\n",
    "fix_params[\"min_child_weight\"] = 4\n",
    "\n",
    "evaluation_list = []\n",
    "for bin in [200, 230, 256, 280]:\n",
    "    params = {**fix_params, **{\"max_bin\": bin}}\n",
    "    evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                        feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "    evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上表可见，默认参数max_bin=256为最优值，但箱子数从200变化到230再变化到256时，对数损失先升后降，并不稳定，于是本文在这个范围内又选取了多个值进行尝试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <td>0.297810</td>\n",
       "      <td>0.297706</td>\n",
       "      <td>0.297846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-logloss-std</th>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.002565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-mean</th>\n",
       "      <td>0.190284</td>\n",
       "      <td>0.190257</td>\n",
       "      <td>0.190366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-std</th>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.001617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <td>0.269347</td>\n",
       "      <td>0.269498</td>\n",
       "      <td>0.269339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-std</th>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-mean</th>\n",
       "      <td>0.169652</td>\n",
       "      <td>0.169738</td>\n",
       "      <td>0.169621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-std</th>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  649       649       649\n",
       "test-logloss-mean            0.297810  0.297706  0.297846\n",
       "test-logloss-std             0.002381  0.002358  0.002565\n",
       "test-weighted_logloss-mean   0.190284  0.190257  0.190366\n",
       "test-weighted_logloss-std    0.001523  0.001496  0.001617\n",
       "train-logloss-mean           0.269347  0.269498  0.269339\n",
       "train-logloss-std            0.000333  0.000609  0.000380\n",
       "train-weighted_logloss-mean  0.169652  0.169738  0.169621\n",
       "train-weighted_logloss-std   0.000242  0.000412  0.000243"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.08\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"subsample\"] = 0.8\n",
    "fix_params[\"colsample_bytree\"] = 0.9\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_depth\"] = 6\n",
    "fix_params[\"min_child_weight\"] = 3.5\n",
    "\n",
    "evaluation_list = []\n",
    "for bin in [220, 240, 270]:\n",
    "    params = {**fix_params, **{\"max_bin\": bin}}\n",
    "    evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                        feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "    evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较两轮调参的结果，选取max_bin=256的确最优。\n",
    "\n",
    "## 3.2.4 调整行采样率subsample和列采样率colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <td>0.297529</td>\n",
       "      <td>0.297085</td>\n",
       "      <td>0.297082</td>\n",
       "      <td>0.297037</td>\n",
       "      <td>0.296903</td>\n",
       "      <td>0.296799</td>\n",
       "      <td>0.297145</td>\n",
       "      <td>0.296989</td>\n",
       "      <td>0.296921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-logloss-std</th>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.002533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-mean</th>\n",
       "      <td>0.190612</td>\n",
       "      <td>0.190429</td>\n",
       "      <td>0.190454</td>\n",
       "      <td>0.190316</td>\n",
       "      <td>0.190253</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.190319</td>\n",
       "      <td>0.190278</td>\n",
       "      <td>0.190279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-std</th>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.001567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <td>0.264996</td>\n",
       "      <td>0.264335</td>\n",
       "      <td>0.263460</td>\n",
       "      <td>0.264254</td>\n",
       "      <td>0.263624</td>\n",
       "      <td>0.263115</td>\n",
       "      <td>0.264729</td>\n",
       "      <td>0.263919</td>\n",
       "      <td>0.263355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-std</th>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-mean</th>\n",
       "      <td>0.166954</td>\n",
       "      <td>0.166541</td>\n",
       "      <td>0.165998</td>\n",
       "      <td>0.166473</td>\n",
       "      <td>0.166099</td>\n",
       "      <td>0.165781</td>\n",
       "      <td>0.166780</td>\n",
       "      <td>0.166298</td>\n",
       "      <td>0.165933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-std</th>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  649       649    ...          649       649\n",
       "test-logloss-mean            0.297529  0.297085    ...     0.296989  0.296921\n",
       "test-logloss-std             0.002477  0.002395    ...     0.002393  0.002533\n",
       "test-weighted_logloss-mean   0.190612  0.190429    ...     0.190278  0.190279\n",
       "test-weighted_logloss-std    0.001469  0.001516    ...     0.001489  0.001567\n",
       "train-logloss-mean           0.264996  0.264335    ...     0.263919  0.263355\n",
       "train-logloss-std            0.000400  0.000652    ...     0.000492  0.000458\n",
       "train-weighted_logloss-mean  0.166954  0.166541    ...     0.166298  0.165933\n",
       "train-weighted_logloss-std   0.000243  0.000434    ...     0.000324  0.000291\n",
       "\n",
       "[8 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_depth\"] = 6\n",
    "fix_params[\"min_child_weight\"] = 4\n",
    "fix_params[\"max_bin\"] = 256\n",
    "\n",
    "evaluation_list = []\n",
    "for row in [0.7, 0.8, 0.9]:\n",
    "    for col in [0.7, 0.8, 0.9]:\n",
    "        params = {**fix_params, **{\"subsample\": row, \"colsample_bytree\": col}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上表可见，第6列的组合最优，行采样率取0.8，列采样率取0.9，进一步构造搜索范围。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "      <th>649</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <td>0.297107</td>\n",
       "      <td>0.297019</td>\n",
       "      <td>0.297064</td>\n",
       "      <td>0.296799</td>\n",
       "      <td>0.296993</td>\n",
       "      <td>0.296911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-logloss-std</th>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-mean</th>\n",
       "      <td>0.190382</td>\n",
       "      <td>0.190382</td>\n",
       "      <td>0.190378</td>\n",
       "      <td>0.190239</td>\n",
       "      <td>0.190357</td>\n",
       "      <td>0.190374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test-weighted_logloss-std</th>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.001555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <td>0.263806</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.263195</td>\n",
       "      <td>0.263115</td>\n",
       "      <td>0.263286</td>\n",
       "      <td>0.262782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-logloss-std</th>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-mean</th>\n",
       "      <td>0.166210</td>\n",
       "      <td>0.165869</td>\n",
       "      <td>0.165825</td>\n",
       "      <td>0.165781</td>\n",
       "      <td>0.165919</td>\n",
       "      <td>0.165601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train-weighted_logloss-std</th>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  649       649    ...          649       649\n",
       "test-logloss-mean            0.297107  0.297019    ...     0.296993  0.296911\n",
       "test-logloss-std             0.002380  0.002376    ...     0.002430  0.002394\n",
       "test-weighted_logloss-mean   0.190382  0.190382    ...     0.190357  0.190374\n",
       "test-weighted_logloss-std    0.001562  0.001462    ...     0.001477  0.001555\n",
       "train-logloss-mean           0.263806  0.263200    ...     0.263286  0.262782\n",
       "train-logloss-std            0.000608  0.000438    ...     0.000518  0.000619\n",
       "train-weighted_logloss-mean  0.166210  0.165869    ...     0.165919  0.165601\n",
       "train-weighted_logloss-std   0.000399  0.000280    ...     0.000328  0.000369\n",
       "\n",
       "[8 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_depth\"] = 6\n",
    "fix_params[\"min_child_weight\"] = 4\n",
    "fix_params[\"max_bin\"] = 256\n",
    "\n",
    "evaluation_list = []\n",
    "for row in [0.75, 0.8, 0.85]:\n",
    "    for col in [0.85, 0.9]:\n",
    "        params = {**fix_params, **{\"subsample\": row, \"colsample_bytree\": col}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优的行采样率为0.8，列采样率为0.9。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.5 调整学习率eta和基学习器数量num_boost_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.657305+4.20501e-05\ttrain-weighted_logloss:0.650569+0.000114187\ttest-logloss:0.657351+0.000175539\ttest-weighted_logloss:0.650612+0.00111213\n",
      "[50]\ttrain-logloss:0.332097+0.000490849\ttrain-weighted_logloss:0.218352+0.000272754\ttest-logloss:0.333685+0.00314723\ttest-weighted_logloss:0.219607+0.0019442\n",
      "[100]\ttrain-logloss:0.318321+0.000419685\ttrain-weighted_logloss:0.200287+0.000284357\ttest-logloss:0.321307+0.00362387\ttest-weighted_logloss:0.20254+0.00221326\n",
      "[150]\ttrain-logloss:0.309791+0.000449484\ttrain-weighted_logloss:0.194623+0.000287813\ttest-logloss:0.314466+0.00367186\ttest-weighted_logloss:0.198093+0.00228025\n",
      "[200]\ttrain-logloss:0.303828+0.000488887\ttrain-weighted_logloss:0.190935+0.000313327\ttest-logloss:0.310269+0.00363093\ttest-weighted_logloss:0.195713+0.00224672\n",
      "[250]\ttrain-logloss:0.299367+0.000402035\ttrain-weighted_logloss:0.188203+0.000256349\ttest-logloss:0.307552+0.00368643\ttest-weighted_logloss:0.194251+0.00227892\n",
      "[300]\ttrain-logloss:0.295766+0.000384017\ttrain-weighted_logloss:0.185966+0.00024855\ttest-logloss:0.30566+0.00362635\ttest-weighted_logloss:0.193251+0.00225564\n",
      "[350]\ttrain-logloss:0.292644+0.000374732\ttrain-weighted_logloss:0.184046+0.000224981\ttest-logloss:0.304163+0.00361395\ttest-weighted_logloss:0.192515+0.00225057\n",
      "[400]\ttrain-logloss:0.289995+0.00042094\ttrain-weighted_logloss:0.182398+0.000261573\ttest-logloss:0.303062+0.00358972\ttest-weighted_logloss:0.191971+0.00223572\n",
      "[450]\ttrain-logloss:0.28745+0.000471596\ttrain-weighted_logloss:0.180835+0.000290805\ttest-logloss:0.302111+0.00350757\ttest-weighted_logloss:0.191555+0.00220593\n",
      "[500]\ttrain-logloss:0.285088+0.000454056\ttrain-weighted_logloss:0.17936+0.000287027\ttest-logloss:0.301338+0.003534\ttest-weighted_logloss:0.191217+0.00220691\n",
      "[550]\ttrain-logloss:0.282961+0.000429834\ttrain-weighted_logloss:0.178054+0.000275456\ttest-logloss:0.300621+0.00353369\ttest-weighted_logloss:0.190938+0.0021953\n",
      "[600]\ttrain-logloss:0.280852+0.000404338\ttrain-weighted_logloss:0.176741+0.000255012\ttest-logloss:0.300002+0.00351149\ttest-weighted_logloss:0.190706+0.00218615\n",
      "[650]\ttrain-logloss:0.278882+0.000439391\ttrain-weighted_logloss:0.175539+0.000289287\ttest-logloss:0.299423+0.00348236\ttest-weighted_logloss:0.190498+0.00215623\n",
      "[700]\ttrain-logloss:0.277022+0.000465309\ttrain-weighted_logloss:0.174387+0.000292374\ttest-logloss:0.298955+0.00344974\ttest-weighted_logloss:0.190342+0.00213591\n",
      "[750]\ttrain-logloss:0.27519+0.000501856\ttrain-weighted_logloss:0.173257+0.000321972\ttest-logloss:0.298513+0.0034736\ttest-weighted_logloss:0.190198+0.00214406\n",
      "[800]\ttrain-logloss:0.273433+0.000463595\ttrain-weighted_logloss:0.172161+0.00030331\ttest-logloss:0.298105+0.00346511\ttest-weighted_logloss:0.190084+0.0021409\n",
      "[850]\ttrain-logloss:0.271754+0.000512424\ttrain-weighted_logloss:0.17112+0.0003202\ttest-logloss:0.297724+0.00341369\ttest-weighted_logloss:0.189979+0.00212021\n",
      "[900]\ttrain-logloss:0.270073+0.000537083\ttrain-weighted_logloss:0.170078+0.000334212\ttest-logloss:0.297371+0.00335884\ttest-weighted_logloss:0.189891+0.0020955\n",
      "[950]\ttrain-logloss:0.268453+0.000514501\ttrain-weighted_logloss:0.169098+0.000319284\ttest-logloss:0.297063+0.00334925\ttest-weighted_logloss:0.189833+0.00209991\n",
      "[1000]\ttrain-logloss:0.266855+0.000514523\ttrain-weighted_logloss:0.168114+0.000319396\ttest-logloss:0.296781+0.00333953\ttest-weighted_logloss:0.189786+0.00209539\n",
      "[1050]\ttrain-logloss:0.265267+0.000463756\ttrain-weighted_logloss:0.16714+0.000292255\ttest-logloss:0.296473+0.00333556\ttest-weighted_logloss:0.189726+0.00208794\n",
      "[1100]\ttrain-logloss:0.263773+0.000470818\ttrain-weighted_logloss:0.166209+0.000304655\ttest-logloss:0.296259+0.00335536\ttest-weighted_logloss:0.189703+0.00209527\n",
      "[1150]\ttrain-logloss:0.262231+0.000484429\ttrain-weighted_logloss:0.165271+0.000305746\ttest-logloss:0.295945+0.00332816\ttest-weighted_logloss:0.189646+0.00209005\n",
      "[1200]\ttrain-logloss:0.26078+0.000444499\ttrain-weighted_logloss:0.164384+0.000283906\ttest-logloss:0.295731+0.00333923\ttest-weighted_logloss:0.189637+0.00209405\n",
      "[1250]\ttrain-logloss:0.259378+0.000469196\ttrain-weighted_logloss:0.163502+0.000287946\ttest-logloss:0.29551+0.00335176\ttest-weighted_logloss:0.189601+0.00210076\n",
      "[1300]\ttrain-logloss:0.257967+0.000471074\ttrain-weighted_logloss:0.162642+0.000297307\ttest-logloss:0.295265+0.0033676\ttest-weighted_logloss:0.189581+0.00211882\n",
      "[1350]\ttrain-logloss:0.256521+0.000511996\ttrain-weighted_logloss:0.161754+0.000309484\ttest-logloss:0.295054+0.00331914\ttest-weighted_logloss:0.189589+0.00210024\n",
      "[1400]\ttrain-logloss:0.255089+0.000497983\ttrain-weighted_logloss:0.160883+0.000318901\ttest-logloss:0.294827+0.00334486\ttest-weighted_logloss:0.189576+0.00210511\n",
      "[1450]\ttrain-logloss:0.253749+0.000485679\ttrain-weighted_logloss:0.160051+0.000308123\ttest-logloss:0.294665+0.00335493\ttest-weighted_logloss:0.189588+0.00211911\n",
      "[1500]\ttrain-logloss:0.252399+0.000512567\ttrain-weighted_logloss:0.159225+0.000323915\ttest-logloss:0.294439+0.00336189\ttest-weighted_logloss:0.189573+0.00211348\n",
      "[1550]\ttrain-logloss:0.251064+0.000491436\ttrain-weighted_logloss:0.158411+0.000314936\ttest-logloss:0.294266+0.00335684\ttest-weighted_logloss:0.189593+0.00210814\n",
      "[1600]\ttrain-logloss:0.249738+0.00045957\ttrain-weighted_logloss:0.157589+0.000297743\ttest-logloss:0.294121+0.00338915\ttest-weighted_logloss:0.189615+0.00212338\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.06\n",
    "params[\"gamma\"] = 0\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 4\n",
    "params[\"max_bin\"] = 256\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "\n",
    "model6 = xgb.cv(params, dtrain, num_boost_round = 6000, nfold = 10, \n",
    "                feval = weighted_log_loss, early_stopping_rounds = 150, \n",
    "                verbose_eval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.668978+2.83304e-05\ttrain-weighted_logloss:0.664471+0.000123119\ttest-logloss:0.669008+0.00011726\ttest-weighted_logloss:0.6645+0.00116836\n",
      "[50]\ttrain-logloss:0.347283+0.000396291\ttrain-weighted_logloss:0.246077+0.000243693\ttest-logloss:0.348415+0.00277397\ttest-weighted_logloss:0.246999+0.0016983\n",
      "[100]\ttrain-logloss:0.325928+0.000434352\ttrain-weighted_logloss:0.208215+0.000271178\ttest-logloss:0.327923+0.00352232\ttest-weighted_logloss:0.209766+0.00215857\n",
      "[150]\ttrain-logloss:0.318607+0.000474931\ttrain-weighted_logloss:0.200456+0.000300074\ttest-logloss:0.321501+0.00365383\ttest-weighted_logloss:0.202646+0.00226829\n",
      "[200]\ttrain-logloss:0.312369+0.000494173\ttrain-weighted_logloss:0.196226+0.000308761\ttest-logloss:0.316369+0.0037101\ttest-weighted_logloss:0.199207+0.00234432\n",
      "[250]\ttrain-logloss:0.30754+0.000357479\ttrain-weighted_logloss:0.193232+0.000230484\ttest-logloss:0.312717+0.00383375\ttest-weighted_logloss:0.197076+0.00240246\n",
      "[300]\ttrain-logloss:0.303814+0.000375371\ttrain-weighted_logloss:0.190935+0.000232996\ttest-logloss:0.310138+0.00381256\ttest-weighted_logloss:0.195621+0.00239132\n",
      "[350]\ttrain-logloss:0.300703+0.000311254\ttrain-weighted_logloss:0.189039+0.000198095\ttest-logloss:0.30819+0.00379849\ttest-weighted_logloss:0.194573+0.00237806\n",
      "[400]\ttrain-logloss:0.298141+0.000350254\ttrain-weighted_logloss:0.187442+0.000213998\ttest-logloss:0.306731+0.00377748\ttest-weighted_logloss:0.193784+0.00236698\n",
      "[450]\ttrain-logloss:0.295748+0.000373497\ttrain-weighted_logloss:0.185969+0.000230872\ttest-logloss:0.305447+0.00372984\ttest-weighted_logloss:0.193128+0.00232647\n",
      "[500]\ttrain-logloss:0.293634+0.000411936\ttrain-weighted_logloss:0.184642+0.000250032\ttest-logloss:0.304408+0.00366594\ttest-weighted_logloss:0.192577+0.00229027\n",
      "[550]\ttrain-logloss:0.291748+0.00039214\ttrain-weighted_logloss:0.18348+0.000242454\ttest-logloss:0.303571+0.00367691\ttest-weighted_logloss:0.192182+0.00229569\n",
      "[600]\ttrain-logloss:0.289887+0.000379735\ttrain-weighted_logloss:0.182331+0.000231349\ttest-logloss:0.302779+0.00366788\ttest-weighted_logloss:0.191812+0.00229212\n",
      "[650]\ttrain-logloss:0.288182+0.000420506\ttrain-weighted_logloss:0.181288+0.000271292\ttest-logloss:0.302108+0.00363095\ttest-weighted_logloss:0.191515+0.00226002\n",
      "[700]\ttrain-logloss:0.28661+0.00041124\ttrain-weighted_logloss:0.180317+0.000263045\ttest-logloss:0.301538+0.00362533\ttest-weighted_logloss:0.191265+0.00225588\n",
      "[750]\ttrain-logloss:0.285075+0.000439274\ttrain-weighted_logloss:0.179363+0.000283615\ttest-logloss:0.300992+0.00363663\ttest-weighted_logloss:0.191027+0.00225565\n",
      "[800]\ttrain-logloss:0.283601+0.000418946\ttrain-weighted_logloss:0.178448+0.000272883\ttest-logloss:0.3005+0.00361624\ttest-weighted_logloss:0.190827+0.00224501\n",
      "[850]\ttrain-logloss:0.2822+0.000445272\ttrain-weighted_logloss:0.177578+0.000279268\ttest-logloss:0.300038+0.00357934\ttest-weighted_logloss:0.190641+0.00222299\n",
      "[900]\ttrain-logloss:0.280835+0.000476263\ttrain-weighted_logloss:0.176735+0.000293722\ttest-logloss:0.299631+0.00354959\ttest-weighted_logloss:0.190486+0.00221161\n",
      "[950]\ttrain-logloss:0.27951+0.000454055\ttrain-weighted_logloss:0.175922+0.000275774\ttest-logloss:0.29925+0.00351685\ttest-weighted_logloss:0.190344+0.00220111\n",
      "[1000]\ttrain-logloss:0.278207+0.000448517\ttrain-weighted_logloss:0.17512+0.000273864\ttest-logloss:0.298914+0.00350735\ttest-weighted_logloss:0.190241+0.0021951\n",
      "[1050]\ttrain-logloss:0.276911+0.000452698\ttrain-weighted_logloss:0.174319+0.000272139\ttest-logloss:0.29855+0.00351112\ttest-weighted_logloss:0.190112+0.00219029\n",
      "[1100]\ttrain-logloss:0.275721+0.000437528\ttrain-weighted_logloss:0.173581+0.000271143\ttest-logloss:0.29825+0.00353621\ttest-weighted_logloss:0.190013+0.00219822\n",
      "[1150]\ttrain-logloss:0.274498+0.000461268\ttrain-weighted_logloss:0.172832+0.000282162\ttest-logloss:0.297952+0.00352765\ttest-weighted_logloss:0.189922+0.00220459\n",
      "[1200]\ttrain-logloss:0.273326+0.000435879\ttrain-weighted_logloss:0.172109+0.000275798\ttest-logloss:0.297659+0.0035036\ttest-weighted_logloss:0.189836+0.00218621\n",
      "[1250]\ttrain-logloss:0.272234+0.000456255\ttrain-weighted_logloss:0.171421+0.000277579\ttest-logloss:0.297415+0.00347936\ttest-weighted_logloss:0.189754+0.00217574\n",
      "[1300]\ttrain-logloss:0.27114+0.000456056\ttrain-weighted_logloss:0.170754+0.00027976\ttest-logloss:0.297162+0.00348656\ttest-weighted_logloss:0.189692+0.00218175\n",
      "[1350]\ttrain-logloss:0.270032+0.000469086\ttrain-weighted_logloss:0.170076+0.000275843\ttest-logloss:0.296922+0.00345889\ttest-weighted_logloss:0.189636+0.00218217\n",
      "[1400]\ttrain-logloss:0.268918+0.00045595\ttrain-weighted_logloss:0.169398+0.000281528\ttest-logloss:0.296665+0.00345181\ttest-weighted_logloss:0.189572+0.00217417\n",
      "[1450]\ttrain-logloss:0.267855+0.000423143\ttrain-weighted_logloss:0.168736+0.000256376\ttest-logloss:0.296451+0.00343901\ttest-weighted_logloss:0.189524+0.00216142\n",
      "[1500]\ttrain-logloss:0.266769+0.00042258\ttrain-weighted_logloss:0.168071+0.000265407\ttest-logloss:0.296204+0.00345183\ttest-weighted_logloss:0.189456+0.0021567\n",
      "[1550]\ttrain-logloss:0.265704+0.000415383\ttrain-weighted_logloss:0.167418+0.000265014\ttest-logloss:0.295995+0.00346804\ttest-weighted_logloss:0.189418+0.00215836\n",
      "[1600]\ttrain-logloss:0.264674+0.000400126\ttrain-weighted_logloss:0.166782+0.000252007\ttest-logloss:0.295819+0.00344999\ttest-weighted_logloss:0.189389+0.00214563\n",
      "[1650]\ttrain-logloss:0.26365+0.000397327\ttrain-weighted_logloss:0.16615+0.000241188\ttest-logloss:0.295646+0.00342373\ttest-weighted_logloss:0.189363+0.00213897\n",
      "[1700]\ttrain-logloss:0.262624+0.000401477\ttrain-weighted_logloss:0.165519+0.000255759\ttest-logloss:0.295468+0.00342865\ttest-weighted_logloss:0.189332+0.00213664\n",
      "[1750]\ttrain-logloss:0.261613+0.000458809\ttrain-weighted_logloss:0.164908+0.000277279\ttest-logloss:0.295291+0.0033937\ttest-weighted_logloss:0.189317+0.00213104\n",
      "[1800]\ttrain-logloss:0.260639+0.000481853\ttrain-weighted_logloss:0.164303+0.000290299\ttest-logloss:0.295134+0.00337822\ttest-weighted_logloss:0.189298+0.00212148\n",
      "[1850]\ttrain-logloss:0.259667+0.000461434\ttrain-weighted_logloss:0.163704+0.00027892\ttest-logloss:0.29499+0.00341286\ttest-weighted_logloss:0.189288+0.0021402\n",
      "[1900]\ttrain-logloss:0.258702+0.000447676\ttrain-weighted_logloss:0.163105+0.000276202\ttest-logloss:0.294833+0.00340786\ttest-weighted_logloss:0.189261+0.00214688\n",
      "[1950]\ttrain-logloss:0.257763+0.000416844\ttrain-weighted_logloss:0.162529+0.000262795\ttest-logloss:0.294682+0.00341925\ttest-weighted_logloss:0.189247+0.00213397\n",
      "[2000]\ttrain-logloss:0.256801+0.000412242\ttrain-weighted_logloss:0.161944+0.000253013\ttest-logloss:0.294526+0.00342034\ttest-weighted_logloss:0.189248+0.00214394\n",
      "[2050]\ttrain-logloss:0.255861+0.000397717\ttrain-weighted_logloss:0.16137+0.000244367\ttest-logloss:0.294386+0.00341982\ttest-weighted_logloss:0.189245+0.00213682\n",
      "[2100]\ttrain-logloss:0.254895+0.000397423\ttrain-weighted_logloss:0.160779+0.000243177\ttest-logloss:0.294232+0.00340015\ttest-weighted_logloss:0.189237+0.00213159\n",
      "[2150]\ttrain-logloss:0.253944+0.000385719\ttrain-weighted_logloss:0.160208+0.000239971\ttest-logloss:0.294073+0.00338952\ttest-weighted_logloss:0.189235+0.00213112\n",
      "[2200]\ttrain-logloss:0.253075+0.000408743\ttrain-weighted_logloss:0.159661+0.000249263\ttest-logloss:0.293958+0.00337963\ttest-weighted_logloss:0.189222+0.00213355\n",
      "[2250]\ttrain-logloss:0.252184+0.000396944\ttrain-weighted_logloss:0.159108+0.000244667\ttest-logloss:0.293849+0.00337462\ttest-weighted_logloss:0.189218+0.00212442\n",
      "[2300]\ttrain-logloss:0.251304+0.000427626\ttrain-weighted_logloss:0.158561+0.000260335\ttest-logloss:0.293731+0.00337637\ttest-weighted_logloss:0.189214+0.00212931\n",
      "[2350]\ttrain-logloss:0.250365+0.000465591\ttrain-weighted_logloss:0.158005+0.000277125\ttest-logloss:0.293572+0.00336378\ttest-weighted_logloss:0.189214+0.00214067\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.04\n",
    "params[\"gamma\"] = 0\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 4\n",
    "params[\"max_bin\"] = 256\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "\n",
    "model4 = xgb.cv(params, dtrain, num_boost_round = 6000, nfold = 10, \n",
    "                feval = weighted_log_loss, early_stopping_rounds = 150, \n",
    "                verbose_eval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.680925+1.43279e-05\ttrain-weighted_logloss:0.678663+0.000132558\ttest-logloss:0.68094+5.8868e-05\ttest-weighted_logloss:0.678678+0.001226\n",
      "[50]\ttrain-logloss:0.409694+0.000270815\ttrain-weighted_logloss:0.337218+0.000176285\ttest-logloss:0.4103+0.0018721\ttest-weighted_logloss:0.337741+0.00102212\n",
      "[100]\ttrain-logloss:0.347244+0.000366838\ttrain-weighted_logloss:0.246864+0.000227617\ttest-logloss:0.348385+0.00279919\ttest-weighted_logloss:0.247795+0.00171622\n",
      "[150]\ttrain-logloss:0.332204+0.000400055\ttrain-weighted_logloss:0.219145+0.000248282\ttest-logloss:0.333776+0.00324428\ttest-weighted_logloss:0.220389+0.00198172\n",
      "[200]\ttrain-logloss:0.326058+0.000419357\ttrain-weighted_logloss:0.208447+0.000264353\ttest-logloss:0.328062+0.0034779\ttest-weighted_logloss:0.210005+0.00213073\n",
      "[250]\ttrain-logloss:0.321934+0.000414153\ttrain-weighted_logloss:0.203407+0.000268534\ttest-logloss:0.324379+0.00361255\ttest-weighted_logloss:0.205281+0.00222427\n",
      "[300]\ttrain-logloss:0.3185+0.000400401\ttrain-weighted_logloss:0.200382+0.000249437\ttest-logloss:0.321402+0.00368126\ttest-weighted_logloss:0.20258+0.00229074\n",
      "[350]\ttrain-logloss:0.315356+0.00042948\ttrain-weighted_logloss:0.198163+0.000266267\ttest-logloss:0.31876+0.00369832\ttest-weighted_logloss:0.200712+0.00231335\n",
      "[400]\ttrain-logloss:0.312308+0.000397239\ttrain-weighted_logloss:0.196181+0.000248995\ttest-logloss:0.316277+0.00374258\ttest-weighted_logloss:0.199149+0.00234551\n",
      "[450]\ttrain-logloss:0.309714+0.000348073\ttrain-weighted_logloss:0.194561+0.000224341\ttest-logloss:0.314258+0.00376168\ttest-weighted_logloss:0.197955+0.00235825\n",
      "[500]\ttrain-logloss:0.30753+0.00034854\ttrain-weighted_logloss:0.193209+0.000226865\ttest-logloss:0.31264+0.00377018\ttest-weighted_logloss:0.19702+0.00235941\n",
      "[550]\ttrain-logloss:0.305553+0.000320825\ttrain-weighted_logloss:0.192003+0.0002117\ttest-logloss:0.311236+0.00381784\ttest-weighted_logloss:0.196236+0.00237479\n",
      "[600]\ttrain-logloss:0.303779+0.000362023\ttrain-weighted_logloss:0.190912+0.000228415\ttest-logloss:0.310048+0.00376502\ttest-weighted_logloss:0.195574+0.00234763\n",
      "[650]\ttrain-logloss:0.302141+0.000366493\ttrain-weighted_logloss:0.189911+0.000235963\ttest-logloss:0.308993+0.00374524\ttest-weighted_logloss:0.194999+0.0023349\n",
      "[700]\ttrain-logloss:0.300666+0.000377368\ttrain-weighted_logloss:0.189003+0.000244581\ttest-logloss:0.30808+0.00372825\ttest-weighted_logloss:0.194502+0.00232209\n",
      "[750]\ttrain-logloss:0.299274+0.000375719\ttrain-weighted_logloss:0.188147+0.000244335\ttest-logloss:0.307263+0.00370869\ttest-weighted_logloss:0.194067+0.00230709\n",
      "[800]\ttrain-logloss:0.298006+0.000372398\ttrain-weighted_logloss:0.187362+0.00024134\ttest-logloss:0.306532+0.00370326\ttest-weighted_logloss:0.193676+0.00230359\n",
      "[850]\ttrain-logloss:0.2968+0.000370888\ttrain-weighted_logloss:0.186616+0.000237648\ttest-logloss:0.305872+0.00368262\ttest-weighted_logloss:0.193333+0.00229393\n",
      "[900]\ttrain-logloss:0.295658+0.000373346\ttrain-weighted_logloss:0.185908+0.000236642\ttest-logloss:0.305248+0.00366617\ttest-weighted_logloss:0.193007+0.00228273\n",
      "[950]\ttrain-logloss:0.294575+0.000379381\ttrain-weighted_logloss:0.185243+0.000236919\ttest-logloss:0.304709+0.00364891\ttest-weighted_logloss:0.192739+0.00227635\n",
      "[1000]\ttrain-logloss:0.293512+0.000340703\ttrain-weighted_logloss:0.184589+0.000220647\ttest-logloss:0.304212+0.00365154\ttest-weighted_logloss:0.192493+0.00227143\n",
      "[1050]\ttrain-logloss:0.29251+0.000347434\ttrain-weighted_logloss:0.183968+0.000219157\ttest-logloss:0.303752+0.00363353\ttest-weighted_logloss:0.192264+0.00225659\n",
      "[1100]\ttrain-logloss:0.291548+0.000343082\ttrain-weighted_logloss:0.18337+0.000226878\ttest-logloss:0.303331+0.00362655\ttest-weighted_logloss:0.192054+0.00224305\n",
      "[1150]\ttrain-logloss:0.290605+0.000349282\ttrain-weighted_logloss:0.182787+0.000227517\ttest-logloss:0.30292+0.00360631\ttest-weighted_logloss:0.191857+0.00223749\n",
      "[1200]\ttrain-logloss:0.28972+0.000325289\ttrain-weighted_logloss:0.182241+0.000214913\ttest-logloss:0.302551+0.00360225\ttest-weighted_logloss:0.191688+0.00223783\n",
      "[1250]\ttrain-logloss:0.288878+0.000337995\ttrain-weighted_logloss:0.181711+0.0002152\ttest-logloss:0.302214+0.00358365\ttest-weighted_logloss:0.191528+0.00223105\n",
      "[1300]\ttrain-logloss:0.288055+0.000345099\ttrain-weighted_logloss:0.181202+0.00022286\ttest-logloss:0.301894+0.00359506\ttest-weighted_logloss:0.191387+0.00223465\n",
      "[1350]\ttrain-logloss:0.287223+0.00033566\ttrain-weighted_logloss:0.180694+0.000206026\ttest-logloss:0.301571+0.00357694\ttest-weighted_logloss:0.191248+0.00223481\n",
      "[1400]\ttrain-logloss:0.286406+0.000339997\ttrain-weighted_logloss:0.180192+0.000222697\ttest-logloss:0.301275+0.00356104\ttest-weighted_logloss:0.191122+0.00222169\n",
      "[1450]\ttrain-logloss:0.285636+0.000343774\ttrain-weighted_logloss:0.179715+0.00022677\ttest-logloss:0.301+0.00356656\ttest-weighted_logloss:0.191+0.00221905\n",
      "[1500]\ttrain-logloss:0.284876+0.000362696\ttrain-weighted_logloss:0.179244+0.000237375\ttest-logloss:0.300735+0.00355498\ttest-weighted_logloss:0.190885+0.00220693\n",
      "[1550]\ttrain-logloss:0.284131+0.000358916\ttrain-weighted_logloss:0.178784+0.000240626\ttest-logloss:0.300491+0.00354437\ttest-weighted_logloss:0.190784+0.00219785\n",
      "[1600]\ttrain-logloss:0.283404+0.000356454\ttrain-weighted_logloss:0.178337+0.00023878\ttest-logloss:0.300253+0.00352785\ttest-weighted_logloss:0.190688+0.00218145\n",
      "[1650]\ttrain-logloss:0.282686+0.000358956\ttrain-weighted_logloss:0.177894+0.000238461\ttest-logloss:0.30002+0.00351133\ttest-weighted_logloss:0.190595+0.0021793\n",
      "[1700]\ttrain-logloss:0.281968+0.000377111\ttrain-weighted_logloss:0.177448+0.000252494\ttest-logloss:0.299783+0.00352108\ttest-weighted_logloss:0.190497+0.00218132\n",
      "[1750]\ttrain-logloss:0.281276+0.000395319\ttrain-weighted_logloss:0.177027+0.000254851\ttest-logloss:0.299555+0.00349596\ttest-weighted_logloss:0.190412+0.00217597\n",
      "[1800]\ttrain-logloss:0.280591+0.000411446\ttrain-weighted_logloss:0.176604+0.00025742\ttest-logloss:0.299349+0.00347632\ttest-weighted_logloss:0.190336+0.00216531\n",
      "[1850]\ttrain-logloss:0.279936+0.000395768\ttrain-weighted_logloss:0.176198+0.000250006\ttest-logloss:0.299155+0.00349239\ttest-weighted_logloss:0.190259+0.00217079\n",
      "[1900]\ttrain-logloss:0.279265+0.000389916\ttrain-weighted_logloss:0.175778+0.000251964\ttest-logloss:0.298968+0.00348982\ttest-weighted_logloss:0.190185+0.00216433\n",
      "[1950]\ttrain-logloss:0.27861+0.000374148\ttrain-weighted_logloss:0.175377+0.000244019\ttest-logloss:0.298781+0.00349984\ttest-weighted_logloss:0.190119+0.00216658\n",
      "[2000]\ttrain-logloss:0.27797+0.000373256\ttrain-weighted_logloss:0.174985+0.000238054\ttest-logloss:0.298606+0.0034979\ttest-weighted_logloss:0.190059+0.00216901\n",
      "[2050]\ttrain-logloss:0.277351+0.00036178\ttrain-weighted_logloss:0.174606+0.000235325\ttest-logloss:0.298423+0.00350117\ttest-weighted_logloss:0.189992+0.00216673\n",
      "[2100]\ttrain-logloss:0.27671+0.000372148\ttrain-weighted_logloss:0.17421+0.000240862\ttest-logloss:0.298255+0.00351181\ttest-weighted_logloss:0.189935+0.00216682\n",
      "[2150]\ttrain-logloss:0.276088+0.000366056\ttrain-weighted_logloss:0.173832+0.00023944\ttest-logloss:0.298079+0.00351527\ttest-weighted_logloss:0.189876+0.00217377\n",
      "[2200]\ttrain-logloss:0.275503+0.000382379\ttrain-weighted_logloss:0.173465+0.000247678\ttest-logloss:0.297945+0.00351993\ttest-weighted_logloss:0.18983+0.00217318\n",
      "[2250]\ttrain-logloss:0.274899+0.000377069\ttrain-weighted_logloss:0.173088+0.000248626\ttest-logloss:0.29779+0.00351932\ttest-weighted_logloss:0.189775+0.00216961\n",
      "[2300]\ttrain-logloss:0.274321+0.000394216\ttrain-weighted_logloss:0.172722+0.000256823\ttest-logloss:0.297658+0.00350902\ttest-weighted_logloss:0.189728+0.00216038\n",
      "[2350]\ttrain-logloss:0.273727+0.000408766\ttrain-weighted_logloss:0.172366+0.000257297\ttest-logloss:0.297497+0.00348731\ttest-weighted_logloss:0.189684+0.00215863\n",
      "[2400]\ttrain-logloss:0.273154+0.000402893\ttrain-weighted_logloss:0.172012+0.000251835\ttest-logloss:0.297347+0.00349787\ttest-weighted_logloss:0.189634+0.00216786\n",
      "[2450]\ttrain-logloss:0.27257+0.000394712\ttrain-weighted_logloss:0.17165+0.000257895\ttest-logloss:0.297208+0.00350065\ttest-weighted_logloss:0.18959+0.00216238\n",
      "[2500]\ttrain-logloss:0.271981+0.000387849\ttrain-weighted_logloss:0.171293+0.000256269\ttest-logloss:0.297055+0.00350419\ttest-weighted_logloss:0.189542+0.00216032\n",
      "[2550]\ttrain-logloss:0.27139+0.000402955\ttrain-weighted_logloss:0.170934+0.00026791\ttest-logloss:0.29691+0.00349432\ttest-weighted_logloss:0.189502+0.00215476\n",
      "[2600]\ttrain-logloss:0.270816+0.000385226\ttrain-weighted_logloss:0.170578+0.000256411\ttest-logloss:0.29679+0.00348992\ttest-weighted_logloss:0.189472+0.0021563\n",
      "[2650]\ttrain-logloss:0.270271+0.000364847\ttrain-weighted_logloss:0.170233+0.000249292\ttest-logloss:0.296679+0.00350486\ttest-weighted_logloss:0.189432+0.0021575\n",
      "[2700]\ttrain-logloss:0.269698+0.000364382\ttrain-weighted_logloss:0.169884+0.000242813\ttest-logloss:0.296552+0.00349632\ttest-weighted_logloss:0.189405+0.0021598\n",
      "[2750]\ttrain-logloss:0.269158+0.000385291\ttrain-weighted_logloss:0.169544+0.000251651\ttest-logloss:0.296448+0.00349181\ttest-weighted_logloss:0.189379+0.00215933\n",
      "[2800]\ttrain-logloss:0.268602+0.000393242\ttrain-weighted_logloss:0.169203+0.000252691\ttest-logloss:0.296332+0.00347848\ttest-weighted_logloss:0.189349+0.00215088\n",
      "[2850]\ttrain-logloss:0.268064+0.000392934\ttrain-weighted_logloss:0.168876+0.000251362\ttest-logloss:0.296213+0.00346885\ttest-weighted_logloss:0.189321+0.00214393\n",
      "[2900]\ttrain-logloss:0.267514+0.000386172\ttrain-weighted_logloss:0.168536+0.000248133\ttest-logloss:0.29609+0.00346953\ttest-weighted_logloss:0.189286+0.00213806\n",
      "[2950]\ttrain-logloss:0.266958+0.000382243\ttrain-weighted_logloss:0.168199+0.000246179\ttest-logloss:0.295968+0.00346072\ttest-weighted_logloss:0.189257+0.00213813\n",
      "[3000]\ttrain-logloss:0.266428+0.000375653\ttrain-weighted_logloss:0.167874+0.000250452\ttest-logloss:0.29586+0.00346541\ttest-weighted_logloss:0.189233+0.00213042\n",
      "[3050]\ttrain-logloss:0.265902+0.000360591\ttrain-weighted_logloss:0.167548+0.000243274\ttest-logloss:0.295753+0.00346616\ttest-weighted_logloss:0.189203+0.00213319\n",
      "[3100]\ttrain-logloss:0.265383+0.00037334\ttrain-weighted_logloss:0.167227+0.000243773\ttest-logloss:0.295638+0.0034436\ttest-weighted_logloss:0.189174+0.00212666\n",
      "[3150]\ttrain-logloss:0.264851+0.00037086\ttrain-weighted_logloss:0.166904+0.000241363\ttest-logloss:0.29553+0.00344666\ttest-weighted_logloss:0.189152+0.00213018\n",
      "[3200]\ttrain-logloss:0.264337+0.000363836\ttrain-weighted_logloss:0.166586+0.000237604\ttest-logloss:0.295431+0.0034445\ttest-weighted_logloss:0.18913+0.00212619\n",
      "[3250]\ttrain-logloss:0.263806+0.000381348\ttrain-weighted_logloss:0.166262+0.00024978\ttest-logloss:0.295325+0.00344471\ttest-weighted_logloss:0.189109+0.00212323\n",
      "[3300]\ttrain-logloss:0.263296+0.000385722\ttrain-weighted_logloss:0.165947+0.000255216\ttest-logloss:0.295229+0.00343987\ttest-weighted_logloss:0.189096+0.00211911\n",
      "[3350]\ttrain-logloss:0.262779+0.000388124\ttrain-weighted_logloss:0.165629+0.000250304\ttest-logloss:0.295136+0.00343026\ttest-weighted_logloss:0.18908+0.00211859\n",
      "[3400]\ttrain-logloss:0.26228+0.000380156\ttrain-weighted_logloss:0.165318+0.000251679\ttest-logloss:0.295052+0.00342907\ttest-weighted_logloss:0.189063+0.00211282\n",
      "[3450]\ttrain-logloss:0.261778+0.000397065\ttrain-weighted_logloss:0.16501+0.000258035\ttest-logloss:0.29496+0.00343342\ttest-weighted_logloss:0.189046+0.00211943\n",
      "[3500]\ttrain-logloss:0.261267+0.000410737\ttrain-weighted_logloss:0.164699+0.000267299\ttest-logloss:0.294865+0.00342789\ttest-weighted_logloss:0.189032+0.00211198\n",
      "[3550]\ttrain-logloss:0.260775+0.000414599\ttrain-weighted_logloss:0.16439+0.000265773\ttest-logloss:0.294775+0.00341778\ttest-weighted_logloss:0.189012+0.00211237\n",
      "[3600]\ttrain-logloss:0.260282+0.000396634\ttrain-weighted_logloss:0.164094+0.000257557\ttest-logloss:0.294681+0.00342859\ttest-weighted_logloss:0.189006+0.00211374\n",
      "[3650]\ttrain-logloss:0.259789+0.000397298\ttrain-weighted_logloss:0.16379+0.000258071\ttest-logloss:0.294598+0.00341563\ttest-weighted_logloss:0.188995+0.00210959\n",
      "[3700]\ttrain-logloss:0.259305+0.000401958\ttrain-weighted_logloss:0.163492+0.000261078\ttest-logloss:0.294519+0.00341339\ttest-weighted_logloss:0.188984+0.00210724\n",
      "[3750]\ttrain-logloss:0.25881+0.000401073\ttrain-weighted_logloss:0.163191+0.000256941\ttest-logloss:0.294427+0.00341048\ttest-weighted_logloss:0.188971+0.00210744\n",
      "[3800]\ttrain-logloss:0.258335+0.000413705\ttrain-weighted_logloss:0.162892+0.000267158\ttest-logloss:0.294362+0.00341562\ttest-weighted_logloss:0.188963+0.00211275\n",
      "[3850]\ttrain-logloss:0.257824+0.000419254\ttrain-weighted_logloss:0.162587+0.000268051\ttest-logloss:0.294265+0.00341266\ttest-weighted_logloss:0.188955+0.00210903\n",
      "[3900]\ttrain-logloss:0.25735+0.000397188\ttrain-weighted_logloss:0.162293+0.000260355\ttest-logloss:0.294185+0.00340949\ttest-weighted_logloss:0.188941+0.00210045\n",
      "[3950]\ttrain-logloss:0.256902+0.000410643\ttrain-weighted_logloss:0.162015+0.000259847\ttest-logloss:0.294111+0.00340099\ttest-weighted_logloss:0.188927+0.00210908\n",
      "[4000]\ttrain-logloss:0.256432+0.000402397\ttrain-weighted_logloss:0.161726+0.000258344\ttest-logloss:0.294037+0.00339397\ttest-weighted_logloss:0.188921+0.00210596\n",
      "[4050]\ttrain-logloss:0.255948+0.000391003\ttrain-weighted_logloss:0.161433+0.000252482\ttest-logloss:0.293962+0.00337869\ttest-weighted_logloss:0.188916+0.0020976\n",
      "[4100]\ttrain-logloss:0.25548+0.000405471\ttrain-weighted_logloss:0.161143+0.000258654\ttest-logloss:0.293884+0.00337547\ttest-weighted_logloss:0.188905+0.00209951\n",
      "[4150]\ttrain-logloss:0.255012+0.000390665\ttrain-weighted_logloss:0.16086+0.000255888\ttest-logloss:0.293815+0.00338325\ttest-weighted_logloss:0.188907+0.00210034\n",
      "[4200]\ttrain-logloss:0.254541+0.000374092\ttrain-weighted_logloss:0.160573+0.000250232\ttest-logloss:0.293731+0.003383\ttest-weighted_logloss:0.188896+0.00209761\n",
      "[4250]\ttrain-logloss:0.254067+0.000389314\ttrain-weighted_logloss:0.160286+0.000248818\ttest-logloss:0.293655+0.00335947\ttest-weighted_logloss:0.188893+0.00209657\n",
      "[4300]\ttrain-logloss:0.253609+0.000393142\ttrain-weighted_logloss:0.160003+0.000259431\ttest-logloss:0.293589+0.00336461\ttest-weighted_logloss:0.188886+0.00208822\n",
      "[4350]\ttrain-logloss:0.253146+0.000390514\ttrain-weighted_logloss:0.159714+0.000250786\ttest-logloss:0.293529+0.00335122\ttest-weighted_logloss:0.188877+0.00208514\n",
      "[4400]\ttrain-logloss:0.252704+0.000382262\ttrain-weighted_logloss:0.159445+0.000252116\ttest-logloss:0.293451+0.00335206\ttest-weighted_logloss:0.18887+0.00207848\n",
      "[4450]\ttrain-logloss:0.252245+0.0003874\ttrain-weighted_logloss:0.159168+0.000252278\ttest-logloss:0.293374+0.00335097\ttest-weighted_logloss:0.188864+0.00207882\n",
      "[4500]\ttrain-logloss:0.251796+0.0003857\ttrain-weighted_logloss:0.15889+0.000251483\ttest-logloss:0.293312+0.00335359\ttest-weighted_logloss:0.188864+0.00207859\n",
      "[4550]\ttrain-logloss:0.25134+0.000385094\ttrain-weighted_logloss:0.15861+0.00025239\ttest-logloss:0.293256+0.00335935\ttest-weighted_logloss:0.188868+0.00208125\n",
      "[4600]\ttrain-logloss:0.250892+0.000396139\ttrain-weighted_logloss:0.158338+0.000255801\ttest-logloss:0.293193+0.00335351\ttest-weighted_logloss:0.188871+0.00208543\n",
      "[4650]\ttrain-logloss:0.250455+0.000390457\ttrain-weighted_logloss:0.158073+0.000252589\ttest-logloss:0.293125+0.00335136\ttest-weighted_logloss:0.188871+0.00208366\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.02\n",
    "params[\"gamma\"] = 0\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 4\n",
    "params[\"max_bin\"] = 256\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "\n",
    "model2 = xgb.cv(params, dtrain, num_boost_round = 6000, nfold = 10, \n",
    "                feval = weighted_log_loss, early_stopping_rounds = 150, \n",
    "                verbose_eval = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 训练最终模型并提交\n",
    "\n",
    "上一节中的模型均在到达允许的最大基学习器数量前早停，观察他们在验证集上的加权对数损失，明显可见当学习率为0.02时最优。验证集的加权对数损失在4450棵树以后开始上升，为了防止过拟合，本文适当减少了基学习器数量，最终同时提交了基学习器数量为3600、3800和4100的模型。\n",
    "\n",
    "**实测无论是在Pubic榜单还是Private榜单中，3600的表现都是最好的，**其Public Scores为0.16898，排名626，Private Scores为0.17358，排名626。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.02\n",
    "params[\"gamma\"] = 0\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 4\n",
    "params[\"max_bin\"] = 256\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "t = pd.read_csv(\"../input/quora-question-pairs/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round = 3600)\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = t[\"test_id\"]\n",
    "sub['is_duplicate'] = prediction\n",
    "sub.to_csv('submission3600.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round = 3800)\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = t[\"test_id\"]\n",
    "sub['is_duplicate'] = prediction\n",
    "sub.to_csv('submission3800.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round = 4100)\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = t[\"test_id\"]\n",
    "sub['is_duplicate'] = prediction\n",
    "sub.to_csv('submission4100.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
